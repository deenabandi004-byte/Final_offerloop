"""
PDL (People Data Labs) client service - search, enrichment, and caching
"""
import requests
import json
import hashlib
import re
from functools import lru_cache
from datetime import datetime
import requests.exceptions
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

from app.config import (
    PEOPLE_DATA_LABS_API_KEY, PDL_BASE_URL, PDL_METRO_AREAS,
    pdl_cache, CACHE_DURATION
)
from app.services.openai_client import get_openai_client
from app.utils.retry import retry_with_backoff

# Create a session with connection pooling for better performance
_session = requests.Session()
_session_lock = Lock()


"""
Enhanced alumni filtering for PDL client - ensures contacts actually attended the school as degree students
"""

def _school_aliases(raw: str) -> list[str]:
    """
    Generate comprehensive aliases for ANY school (not just USC/Stanford)
    Returns list of normalized variations that PDL might use
    """
    if not raw:
        return []
    
    # Clean and normalize input
    s = " ".join(str(raw).lower().split())
    aliases = {s}
    
    # Remove common words to get core name
    core = s
    for remove in [" university", " college", " school", " institute", " of technology", ", the"]:
        if remove in core:
            core = core.replace(remove, "").strip()
    
    if core and core != s:
        aliases.add(core)
    
    # Add "University of X" variants
    if not s.startswith("university of") and core:
        aliases.add(f"university of {core}")
    
    # Add variations with/without "the"
    if s.startswith("the "):
        aliases.add(s[4:])  # Remove "the"
    else:
        aliases.add(f"the {s}")
    
    # Comprehensive school-specific aliases map
    school_map = {
        # California schools
        "usc": ["university of southern california", "usc viterbi", "viterbi school of engineering", "southern california"],
        "university of southern california": ["usc", "usc viterbi", "viterbi school of engineering", "southern california"],
        "southern california": ["usc", "university of southern california", "usc viterbi"],
        
        "ucla": ["university of california los angeles", "university of california, los angeles", "uc los angeles"],
        "university of california los angeles": ["ucla", "uc los angeles", "cal los angeles"],
        
        "berkeley": ["uc berkeley", "university of california berkeley", "cal", "california berkeley"],
        "university of california berkeley": ["berkeley", "uc berkeley", "cal"],
        
        "ucsd": ["uc san diego", "university of california san diego", "california san diego"],
        "ucsi": ["uc irvine", "university of california irvine", "california irvine"],
        "ucsb": ["uc santa barbara", "university of california santa barbara", "california santa barbara"],
        
        "stanford": ["stanford university", "leland stanford junior university"],
        "stanford university": ["stanford", "leland stanford junior university"],
        
        "caltech": ["california institute of technology", "cal tech"],
        
        # Ivy League
        "harvard": ["harvard university", "harvard college"],
        "harvard university": ["harvard", "harvard college"],
        
        "yale": ["yale university"],
        "yale university": ["yale"],
        
        "princeton": ["princeton university"],
        "princeton university": ["princeton"],
        
        "columbia": ["columbia university", "columbia university in the city of new york"],
        "columbia university": ["columbia", "columbia university in the city of new york"],
        
        "penn": ["university of pennsylvania", "upenn", "wharton"],
        "university of pennsylvania": ["penn", "upenn", "wharton"],
        
        "brown": ["brown university"],
        "brown university": ["brown"],
        
        "dartmouth": ["dartmouth college", "dartmouth university"],
        "dartmouth college": ["dartmouth", "dartmouth university"],
        
        "cornell": ["cornell university"],
        "cornell university": ["cornell"],
        
        # Other top schools
        "mit": ["massachusetts institute of technology", "m.i.t."],
        "massachusetts institute of technology": ["mit", "m.i.t."],
        
        "nyu": ["new york university", "new york u"],
        "new york university": ["nyu", "new york u"],
        
        "duke": ["duke university"],
        "duke university": ["duke"],
        
        "northwestern": ["northwestern university"],
        "northwestern university": ["northwestern"],
        
        "chicago": ["university of chicago", "uchicago", "u of chicago"],
        "university of chicago": ["chicago", "uchicago", "u of chicago"],
        
        "michigan": ["university of michigan", "umich", "u of michigan", "michigan ann arbor"],
        "university of michigan": ["michigan", "umich", "u of michigan", "michigan ann arbor"],
        
        "virginia": ["university of virginia", "uva", "u of virginia"],
        "university of virginia": ["virginia", "uva", "u of virginia"],
        
        "notre dame": ["university of notre dame", "the university of notre dame"],
        "university of notre dame": ["notre dame", "the university of notre dame"],
        
        "carnegie mellon": ["carnegie mellon university", "cmu"],
        "carnegie mellon university": ["carnegie mellon", "cmu"],
        
        "georgia tech": ["georgia institute of technology", "georgia tech"],
        "georgia institute of technology": ["georgia tech"],
        
        "texas": ["university of texas", "ut austin", "texas austin"],
        "university of texas": ["texas", "ut austin", "texas austin"],
        
        "washington": ["university of washington", "uw", "washington seattle"],
        "university of washington": ["washington", "uw", "washington seattle"],
        
        "wisconsin": ["university of wisconsin", "uw madison", "wisconsin madison"],
        "university of wisconsin": ["wisconsin", "uw madison", "wisconsin madison"],
        
        "illinois": ["university of illinois", "uiuc", "illinois urbana champaign"],
        "university of illinois": ["illinois", "uiuc", "illinois urbana champaign"],
        
        # Add more as needed
    }
    
    # Check if any key matches and add expansions
    for key, expansions in school_map.items():
        if key in s or any(key in alias for alias in list(aliases)):
            aliases.update(expansions)
            aliases.add(key)  # Also add the short form
    
    # Clean up and return sorted
    cleaned = {" ".join(a.split()).strip() for a in aliases if a and len(a.strip()) > 1}
    return sorted(cleaned)


def _contact_has_school_as_primary_education(contact: dict, aliases: list[str]) -> bool:
    """
    ENHANCED VERSION: Check if contact has the school as their PRIMARY education (degree-granting)
    Returns True only if the school appears to be where they got their main degree
    """
    # Check if we have detailed education data
    edu = contact.get("education") or []
    
    if isinstance(edu, list) and edu:
        # Sort education by importance (degrees, end dates, etc.)
        primary_schools = []
        
        for e in edu:
            if isinstance(e, dict):
                school = e.get("school") or {}
                school_name = ""
                
                if isinstance(school, dict):
                    school_name = (school.get("name") or "").lower()
                elif isinstance(e.get("school"), str):
                    school_name = e.get("school", "").lower()
                
                # Check if this education entry has degree indicators
                has_degree = False
                degree_fields = e.get("degrees") or []
                
                # Check for degree indicators
                if degree_fields:  # Has explicit degree field
                    has_degree = True
                elif e.get("degree"):  # Alternative degree field
                    has_degree = True
                elif e.get("end_date"):  # Has completion date (suggests full program)
                    has_degree = True
                elif any(deg in school_name for deg in ["university", "college", "institute"]):
                    # If it looks like a degree-granting institution and has other indicators
                    if e.get("start_date") or e.get("field_of_study") or e.get("major"):
                        has_degree = True
                
                # If this looks like a degree-granting education, add to primary schools
                if has_degree and school_name:
                    primary_schools.append(school_name)
        
        # Check if any primary schools match our aliases
        for school_name in primary_schools:
            for alias in aliases:
                if alias in school_name or school_name in alias:
                    return True
    
    # Fallback: Check top-level education fields
    # But weight them lower since they might be less reliable
    edu_top = (contact.get("EducationTop") or "").lower()
    college = (contact.get("College") or contact.get("college") or "").lower()
    
    # Only use these if they look like primary education
    primary_indicators = ["bachelor", "master", "phd", "mba", "degree", "graduated", "alumni"]
    
    for field in [edu_top, college]:
        if field:
            # Check if this field contains both the school AND degree indicators
            has_degree_indicator = any(indicator in field for indicator in primary_indicators)
            
            for alias in aliases:
                if alias in field:
                    # If we have degree indicators OR the field is specifically the "College" field
                    if has_degree_indicator or field == college:
                        return True
    
    return False
def _contact_has_school_as_primary_education_lenient(contact: dict, aliases: list[str]) -> bool:
    """
    MORE LENIENT VERSION - Accept if school appears in education
    Fixed bidirectional substring matching
    """
    # Check if we have detailed education data
    edu = contact.get("education") or []
    
    if isinstance(edu, list) and edu:
        for e in edu:
            if isinstance(e, dict):
                school = e.get("school") or {}
                school_name = ""
                
                if isinstance(school, dict):
                    school_name = (school.get("name") or "").lower()
                elif isinstance(e.get("school"), str):
                    school_name = e.get("school", "").lower()
                
                # ‚úÖ VERY LENIENT: Accept if school name is substantial
                if school_name and len(school_name) > 2:
                    for alias in aliases:
                        # ‚úÖ FIX: Bidirectional substring check
                        if alias in school_name or school_name in alias:
                            # Accept if ANY of these exist (even empty values)
                            has_any_indicator = (
                                "degrees" in e or
                                "degree" in e or
                                "end_date" in e or
                                "start_date" in e or
                                "field_of_study" in e or
                                "major" in e or
                                len(school_name) > 5  # Substantial school name = likely real
                            )
                            if has_any_indicator:
                                return True
    
    # Also check College field (often reliable for primary degree)
    college = (contact.get("College") or contact.get("college") or "").lower()
    if college and len(college) > 2:
        for alias in aliases:
            # ‚úÖ FIX: Bidirectional check
            if alias in college or college in alias:
                return True  # Trust the College field
    
    # Check EducationTop
    edu_top = (contact.get("EducationTop") or "").lower()
    if edu_top and len(edu_top) > 2:
        for alias in aliases:
            # ‚úÖ FIX: Bidirectional check
            if alias in edu_top or edu_top in alias:
                return True
    
    return False
def _contact_hash(contact: dict) -> tuple:
    """Generate a tuple of key fields to identify a contact uniquely"""
    first = (contact.get("FirstName") or "").lower().strip()
    last = (contact.get("LastName") or "").lower().strip()
    email = (contact.get("Email") or "").lower().strip()
    company = (contact.get("Company") or "").lower().strip()
    
    # ALWAYS use (first, last, company) for identity
    # Email can be added later (PDL‚ÜíHunter), so it's not reliable for deduplication
    return (first, last, company)
def get_contact_identity(contact: dict) -> str:
    """Generate a unique identity string for a contact"""
    return "||".join(_contact_hash(contact))
def _fetch_verified_alumni_contacts(
    primary_title, similar_titles, cleaned_company,
    location_strategy, job_title_enrichment,
    max_contacts, college_alumni, excluded_keys=None  # ADD excluded_keys parameter
):
    """
    Fetch contacts in batches until we have enough verified alumni
    Guarantees to return the requested number if they exist
    """
    aliases = _school_aliases(college_alumni)
    if not aliases:
        print(f"‚ö†Ô∏è No aliases found for {college_alumni}")
        return []
    
    excluded_keys = excluded_keys or set()  # ADD this line
    verified_alumni = []
    all_fetched_contacts = []
    batch_size = max_contacts * 2  # Start with 2x the requested amount
    max_total_fetch = 200  # Increased to 200 to handle low alumni rates (e.g., 10% = 20 alumni from 200 contacts)
    total_fetched = 0
    attempts = 0
    max_attempts = 4
    
    print(f"üéì Starting alumni search for {max_contacts} verified {college_alumni} graduates")
    print(f"   Excluding {len(excluded_keys)} previously seen contacts")  # ADD this line
    
    while len(verified_alumni) < max_contacts and attempts < max_attempts and total_fetched < max_total_fetch:
        attempts += 1
        
        # Calculate how many to fetch this round
        current_batch_size = min(batch_size, max_total_fetch - total_fetched)
        if current_batch_size <= 0:
            break
            
        print(f"\nüì• Attempt {attempts}: Fetching {current_batch_size} contacts...")
        
        # Fetch contacts using the appropriate strategy
        if location_strategy['strategy'] == 'metro_primary':
            batch_contacts = try_metro_search_optimized(
                primary_title, similar_titles, cleaned_company,
                location_strategy, current_batch_size,
                college_alumni=college_alumni,
                exclude_keys=excluded_keys  # ADD this parameter
            )
            
            # If not enough, try locality
            if len(batch_contacts) < current_batch_size:
                locality_contacts = try_locality_search_optimized(
                    primary_title, similar_titles, cleaned_company,
                    location_strategy, current_batch_size - len(batch_contacts),
                    college_alumni=college_alumni,
                    exclude_keys=excluded_keys  # ADD this parameter
                )
                batch_contacts.extend([c for c in locality_contacts if c not in batch_contacts])
        else:
            batch_contacts = try_locality_search_optimized(
                primary_title, similar_titles, cleaned_company,
                location_strategy, current_batch_size,
                college_alumni=college_alumni,
                exclude_keys=excluded_keys  # ADD this parameter
            )
            
            # If not enough, try broader search
            if len(batch_contacts) < current_batch_size:
                broader_contacts = try_job_title_levels_search_enhanced(
                    job_title_enrichment, cleaned_company,
                    location_strategy['city'], location_strategy['state'],
                    current_batch_size - len(batch_contacts),
                    college_alumni=college_alumni,
                    exclude_keys=excluded_keys  # ADD this parameter
                )
                batch_contacts.extend([c for c in broader_contacts if c not in batch_contacts])
        
        # Track what we've fetched (with duplicate prevention)
        for contact in batch_contacts:
            if contact not in all_fetched_contacts:
                all_fetched_contacts.append(contact)
        
        total_fetched += len(batch_contacts)
        
        # Apply strict alumni filter to new batch
        # IMPORTANT: Check alumni status FIRST, then exclude duplicates
        new_verified = 0
        for contact in batch_contacts:
            # First verify if they're alumni
            if _contact_has_school_as_primary_education_lenient(contact, aliases):
                # Only now check if we've already added them (by identity, not exclude_keys)
                contact_key = get_contact_identity(contact)
                
                # Skip if already in verified_alumni (duplicate check)
                already_added = any(get_contact_identity(v) == contact_key for v in verified_alumni)
                if already_added:
                    continue
                
                # Skip if in excluded_keys (user's contact library)
                if contact_key in excluded_keys:
                    continue
                
                # Add to verified alumni
                verified_alumni.append(contact)
                new_verified += 1
                
                # Log each verified alumni found
                name = f"{contact.get('FirstName', '')} {contact.get('LastName', '')}".strip()
                print(f"   ‚úì Verified alumni #{len(verified_alumni)}: {name}")
                    
                if len(verified_alumni) >= max_contacts:
                    break
        
        print(f"   Found {new_verified} new verified alumni from {len(batch_contacts)} contacts")
        print(f"   Total verified: {len(verified_alumni)}/{max_contacts}")
        
        # If we're not finding many alumni, increase batch size for next attempt
        if new_verified < batch_size * 0.2:  # Less than 20% success rate
            batch_size = min(batch_size * 2, 25)  # Double the batch size, cap at 25
            print(f"   üìà Low alumni rate, increasing next batch size to {batch_size}")
    
    # Final summary
    print(f"\nüéì Alumni search complete:")
    print(f"   Total contacts fetched: {total_fetched}")
    print(f"   Total verified {college_alumni} alumni: {len(verified_alumni)}")
    
    if len(verified_alumni) < max_contacts:
        print(f"   ‚ö†Ô∏è Only found {len(verified_alumni)} verified alumni (requested {max_contacts})")
        print(f"   Consider broadening search criteria or removing company/location filters")
    else:
        print(f"   ‚úÖ Successfully found {max_contacts} verified alumni")
    
    return verified_alumni[:max_contacts]



def _fetch_contacts_standard(
    primary_title, similar_titles, cleaned_company,
    location_strategy, job_title_enrichment,
    max_contacts, exclude_keys=None
):
    """
    Standard contact fetching without alumni filter (original logic)
    DEPRECATED: Use _fetch_contacts_standard_parallel for better performance
    """
    excluded_keys = exclude_keys or set()
    contacts = []
    
    if location_strategy['strategy'] == 'metro_primary':
        contacts = try_metro_search_optimized(
            primary_title, similar_titles, cleaned_company,
            location_strategy, max_contacts,
            college_alumni=None,
            exclude_keys=excluded_keys  # ‚úÖ PASS EXCLUDE_KEYS
        )
        
        if len(contacts) < max_contacts:
            print(f"Metro results insufficient ({len(contacts)}), adding locality results")
            locality_contacts = try_locality_search_optimized(
                primary_title, similar_titles, cleaned_company,
                location_strategy, max_contacts - len(contacts),
                college_alumni=None,
                exclude_keys=excluded_keys
            )
            contacts.extend([c for c in locality_contacts if c not in contacts])
    else:
        contacts = try_locality_search_optimized(
            primary_title, similar_titles, cleaned_company,
            location_strategy, max_contacts,
            college_alumni=None,
            exclude_keys=excluded_keys
        )
        
        if len(contacts) < max_contacts:
            print(f"Locality results insufficient ({len(contacts)}), trying broader search")
            broader_contacts = try_job_title_levels_search_enhanced(
                job_title_enrichment, cleaned_company,
                location_strategy['city'], location_strategy['state'],
                max_contacts - len(contacts),
                college_alumni=None,
                exclude_keys=excluded_keys  # ‚úÖ PASS EXCLUDE_KEYS
            )
            contacts.extend([c for c in broader_contacts if c not in contacts])
    
    return contacts


def _fetch_contacts_standard_parallel(
    primary_title, similar_titles, cleaned_company,
    location_strategy, job_title_enrichment,
    max_contacts, exclude_keys=None
):
    """
    OPTIMIZED: Parallel contact fetching - runs multiple search strategies simultaneously
    This significantly reduces search time by trying all strategies in parallel
    """
    excluded_keys = exclude_keys or set()
    contacts = []
    seen_identities = set()
    
    import time
    search_start = time.time()
    
    # Determine which searches to run based on location strategy
    searches_to_run = []
    
    if location_strategy['strategy'] == 'metro_primary':
        # Run metro and locality searches in parallel
        searches_to_run.append(('metro', lambda: try_metro_search_optimized(
            primary_title, similar_titles, cleaned_company,
            location_strategy, max_contacts,
            college_alumni=None,
            exclude_keys=excluded_keys
        )))
        searches_to_run.append(('locality', lambda: try_locality_search_optimized(
            primary_title, similar_titles, cleaned_company,
            location_strategy, max_contacts,
            college_alumni=None,
            exclude_keys=excluded_keys
        )))
    else:
        # Run locality search first
        searches_to_run.append(('locality', lambda: try_locality_search_optimized(
            primary_title, similar_titles, cleaned_company,
            location_strategy, max_contacts,
            college_alumni=None,
            exclude_keys=excluded_keys
        )))
    
    # Always prepare broader search as backup
    broader_search = lambda: try_job_title_levels_search_enhanced(
        job_title_enrichment, cleaned_company,
        location_strategy['city'], location_strategy['state'],
        max_contacts,
        college_alumni=None,
        exclude_keys=excluded_keys
    )
    
    # Run initial searches in parallel
    if searches_to_run:
        print(f"‚ö° Running {len(searches_to_run)} search strategies in parallel...")
        with ThreadPoolExecutor(max_workers=len(searches_to_run)) as executor:
            futures = {executor.submit(search_func): name for name, search_func in searches_to_run}
            
            for future in as_completed(futures):
                search_name = futures[future]
                try:
                    results = future.result()
                    print(f"‚úÖ {search_name} search returned {len(results)} contacts")
                    
                    # Deduplicate by identity
                    for contact in results:
                        contact_id = get_contact_identity(contact)
                        if contact_id not in seen_identities:
                            contacts.append(contact)
                            seen_identities.add(contact_id)
                            if len(contacts) >= max_contacts:
                                break
                except Exception as e:
                    print(f"‚ùå {search_name} search failed: {e}")
    
    # If we still need more contacts, try broader search
    if len(contacts) < max_contacts:
        needed = max_contacts - len(contacts)
        print(f"‚ö° Running broader search for {needed} more contacts...")
        try:
            broader_contacts = broader_search()
            for contact in broader_contacts:
                contact_id = get_contact_identity(contact)
                if contact_id not in seen_identities:
                    contacts.append(contact)
                    seen_identities.add(contact_id)
                    if len(contacts) >= max_contacts:
                        break
        except Exception as e:
            print(f"‚ùå Broader search failed: {e}")
    
    search_time = time.time() - search_start
    print(f"‚ö° Parallel search completed in {search_time:.2f}s - found {len(contacts)} contacts")
    
    return contacts[:max_contacts]

def _contact_has_school_alias(c: dict, aliases: list[str]) -> bool:
    """
    ORIGINAL VERSION - kept for backward compatibility
    Check if contact has any of the school aliases in their education (loose matching)
    """
    fields = []
    fields.append((c.get("College") or c.get("college") or "").lower())
    edu = c.get("education") or []
    if isinstance(edu, list):
        for e in edu:
            if isinstance(e, dict):
                school = e.get("school") or {}
                if isinstance(school, dict):
                    fields.append((school.get("name") or "").lower())
            elif isinstance(e, str):
                fields.append(e.lower())
    elif isinstance(edu, str):
        fields.append(edu.lower())
    
    edu_top = (c.get("EducationTop") or "").lower()
    if edu_top:
        fields.append(edu_top)
    
    for field in fields:
        for alias in aliases:
            if alias in field or field in alias:
                return True
    return False

def _contact_has_school_as_primary_education(contact: dict, aliases: list[str]) -> bool:
    """
    Enhanced version: Check if contact has the school as their PRIMARY education (degree-granting)
    Returns True only if the school appears to be where they got their main degree
    """
    # Check if we have detailed education data
    edu = contact.get("education") or []
    
    if isinstance(edu, list) and edu:
        # Look for degree-granting education entries
        for e in edu:
            if isinstance(e, dict):
                school = e.get("school") or {}
                school_name = ""
                
                if isinstance(school, dict):
                    school_name = (school.get("name") or "").lower()
                elif isinstance(e.get("school"), str):
                    school_name = e.get("school", "").lower()
                
                # Check if this education entry has degree indicators
                has_degree = False
                
                # Check for explicit degree fields
                if e.get("degrees") or e.get("degree"):
                    has_degree = True
                # Check for graduation/completion indicators  
                elif e.get("end_date") and e.get("start_date"):
                    has_degree = True
                # Check for field of study (usually indicates degree program)
                elif e.get("field_of_study") or e.get("major"):
                    has_degree = True
                
                # If this looks like a degree-granting education, check against aliases
                if has_degree and school_name:
                    for alias in aliases:
                        if alias in school_name or school_name in alias:
                            print(f"‚úì Verified {contact.get('FirstName', '')} {contact.get('LastName', '')} has degree from {school_name}")
                            return True
    
    # Fallback: Check College field (usually indicates primary degree)
    college = (contact.get("College") or contact.get("college") or "").lower()
    if college:
        for alias in aliases:
            if alias in college:
                return True
    
    # Don't use EducationTop alone as it might include certificates/courses
    return False
def apply_strict_alumni_filter(contacts: list, college_alumni: str, use_strict: bool = True) -> list:
    """
    Apply alumni filtering with option for strict or loose matching
    
    Args:
        contacts: List of contact dictionaries
        college_alumni: School name to filter by
        use_strict: If True, use strict degree-based filtering. If False, use original loose matching.
    
    Returns:
        Filtered list of contacts who are actual alumni
    """
    if not college_alumni:
        return contacts
    
    aliases = _school_aliases(college_alumni)
    if not aliases:
        return contacts
    
    if use_strict:
        # Use enhanced filtering that checks for actual degrees
        filtered = [c for c in contacts if _contact_has_school_as_primary_education(c, aliases)]
        
        # Log the filtering results
        original_count = len(contacts)
        filtered_count = len(filtered)
        if original_count > filtered_count:
            print(f"üéì Strict alumni filter: {original_count} ‚Üí {filtered_count} contacts")
            print(f"   Removed {original_count - filtered_count} contacts without confirmed {college_alumni} degrees")
        
        return filtered
    else:
        # Use original loose filtering
        return [c for c in contacts if _contact_has_school_alias(c, aliases)]


# Enhanced PDL query builder for alumni search
def build_enhanced_alumni_query(aliases: list[str]) -> dict:
    """
    Build a more sophisticated alumni query that prioritizes actual degree holders
    
    This query uses boosting to rank actual alumni higher while still catching edge cases
    """
    should_clauses = []
    
    for alias in aliases:
        # High boost for full school name in education
        should_clauses.append({
            "match_phrase": {
                "education.school.name": {
                    "query": alias,
                    "boost": 3.0  # High priority
                }
            }
        })
        
        # Medium boost for school + degree indicators
        degree_terms = ["bachelor", "bs", "ba", "master", "ms", "ma", "mba", "phd", "degree"]
        for degree in degree_terms:
            should_clauses.append({
                "bool": {
                    "must": [
                        {"match_phrase": {"education.school.name": alias}},
                        {"match": {"education.degrees": degree}}
                    ],
                    "boost": 5.0  # Very high priority for confirmed degrees
                }
            })
        
        # Lower boost for school mentions without degree confirmation
        should_clauses.append({
            "match": {
                "education.summary": {
                    "query": alias,
                    "boost": 0.5  # Low priority
                }
            }
        })
    
    return {"bool": {"should": should_clauses}}


# Integration point for your existing code
def search_contacts_with_smart_location_strategy_enhanced(
    job_title, company, location, max_contacts=8, college_alumni=None, 
    use_strict_alumni_filter=True
):
    """
    Enhanced version of your existing search function with better alumni filtering
    
    This is a wrapper that would call your existing function and apply enhanced filtering
    """
    # Import your existing function (adjust import as needed)
    from app.services.pdl_client import search_contacts_with_smart_location_strategy
    
    # Call existing search function
    contacts = search_contacts_with_smart_location_strategy(
        job_title, company, location, max_contacts=max_contacts * 2,  # Get extra to account for filtering
        college_alumni=college_alumni  # Still pass for query-level filtering
    )
    
    # Apply enhanced post-processing filter
    if college_alumni and use_strict_alumni_filter:
        contacts = apply_strict_alumni_filter(contacts, college_alumni, use_strict=True)
    
    # Return up to max_contacts
    return contacts[:max_contacts]


# Example usage and testing
def test_alumni_filtering():
    """
    Test function to demonstrate the difference between loose and strict filtering
    """
    # Example contact that went to Rutgers but has Stanford certificate
    test_contact = {
        "FirstName": "Ismael",
        "LastName": "Menjivar",
        "education": [
            {
                "school": {"name": "Rutgers University"},
                "degrees": ["Bachelor of Science"],
                "field_of_study": "Computer Science",
                "end_date": "2018"
            },
            {
                "school": {"name": "Stanford University"},
                "summary": "Online Certificate in Machine Learning",
                "end_date": "2020"
            }
        ],
        "EducationTop": "Rutgers University",
        "College": "Rutgers"
    }
    
    stanford_aliases = _school_aliases("Stanford University")
    
    # Test original loose matching
    has_stanford_loose = _contact_has_school_alias(test_contact, stanford_aliases)
    print(f"Loose matching: {has_stanford_loose}")  # Would return True (incorrect)
    
    # Test new strict matching
    has_stanford_strict = _contact_has_school_as_primary_education(test_contact, stanford_aliases)
    print(f"Strict matching: {has_stanford_strict}")  # Would return False (correct)
    
    return test_contact


if __name__ == "__main__":
    # Test the filtering
    test_alumni_filtering()


def _contact_has_school_alias(c: dict, aliases: list[str]) -> bool:
    """Check if contact has any of the school aliases in their education"""
    fields = []
    fields.append((c.get("College") or c.get("college") or "").lower())
    edu = c.get("education") or []
    if isinstance(edu, list):
        for e in edu:
            if isinstance(e, dict):
                school = e.get("school") or {}
                if isinstance(school, dict):
                    fields.append((school.get("name") or "").lower())
            elif isinstance(e, str):
                fields.append(e.lower())
    elif isinstance(edu, str):
        fields.append(edu.lower())
    
    edu_top = (c.get("EducationTop") or "").lower()
    if edu_top:
        fields.append(edu_top)
    
    for field in fields:
        for alias in aliases:
            if alias in field or field in alias:
                return True
    return False


@lru_cache(maxsize=1000)
def cached_enrich_job_title(job_title):
    """Cache job title enrichments to avoid repeated API calls"""
    return enrich_job_title_with_pdl(job_title)


@lru_cache(maxsize=1000)
def cached_clean_company(company):
    """Cache company cleaning to avoid repeated API calls"""
    return clean_company_name(company) if company else ''


@lru_cache(maxsize=1000)
def cached_clean_location(location):
    """Cache location cleaning to avoid repeated API calls"""
    return clean_location_name(location) if location else ''


def clean_company_name(company):
    """Clean company name using PDL Cleaner API for better matching
    OPTIMIZED: Uses connection pooling for better performance
    """
    try:
        print(f"Cleaning company name: {company}")
        
        # Use session for connection pooling
        with _session_lock:
            response = _session.get(
                f"{PDL_BASE_URL}/company/clean",
                params={
                    'api_key': PEOPLE_DATA_LABS_API_KEY,
                    'name': company
                },
                timeout=10
            )
        
        if response.status_code == 200:
            clean_data = response.json()
            if clean_data.get('status') == 200 and clean_data.get('name'):
                cleaned_name = clean_data['name']
                print(f"Cleaned company: '{company}' -> '{cleaned_name}'")
                return cleaned_name
    
    except Exception as e:
        print(f"Company cleaning failed: {e}")
    
    return company


# US State abbreviations mapping
US_STATE_ABBREVIATIONS = {
    'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas',
    'CA': 'California', 'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware',
    'FL': 'Florida', 'GA': 'Georgia', 'HI': 'Hawaii', 'ID': 'Idaho',
    'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa', 'KS': 'Kansas',
    'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',
    'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi',
    'MO': 'Missouri', 'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada',
    'NH': 'New Hampshire', 'NJ': 'New Jersey', 'NM': 'New Mexico', 'NY': 'New York',
    'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio', 'OK': 'Oklahoma',
    'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina',
    'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah',
    'VT': 'Vermont', 'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia',
    'WI': 'Wisconsin', 'WY': 'Wyoming', 'DC': 'District of Columbia'
}

def _expand_us_state_abbreviation(location: str) -> str:
    """
    Expand US state abbreviations in location strings to avoid confusion with country codes.
    Example: "Research Park, CA" -> "Research Park, California"
    Handles formats like:
    - "City, ST" -> "City, State"
    - "City, ST 12345" -> "City, State"
    - "City ST" -> "City, State"
    """
    if not location:
        return location
    
    # Most common pattern: "City, ST" or "City, ST ZIP"
    # Match ", ST" or ", ST ZIP" at the end
    comma_pattern = r',\s*([A-Z]{2})(?:\s+\d{5})?$'
    
    def replace_comma_state(match):
        state_abbr = match.group(1).upper()
        if state_abbr in US_STATE_ABBREVIATIONS:
            return f', {US_STATE_ABBREVIATIONS[state_abbr]}'
        return match.group(0)
    
    expanded = re.sub(comma_pattern, replace_comma_state, location, flags=re.IGNORECASE)
    
    # If no comma pattern matched, try "City ST" (without comma)
    if expanded == location:
        # Match "City ST" or "City ST ZIP" at the end
        no_comma_pattern = r'^(.+?)\s+([A-Z]{2})(?:\s+\d{5})?$'
        
        def replace_no_comma_state(match):
            city = match.group(1).strip()
            state_abbr = match.group(2).upper()
            if state_abbr in US_STATE_ABBREVIATIONS:
                return f'{city}, {US_STATE_ABBREVIATIONS[state_abbr]}'
            return match.group(0)
        
        expanded = re.sub(no_comma_pattern, replace_no_comma_state, location, flags=re.IGNORECASE)
    
    return expanded if expanded != location else location


def _fix_canada_misinterpretation(original_location: str, cleaned_location: str) -> str:
    """
    Fix cases where PDL incorrectly interprets US state "CA" (California) as Canada.
    If the original location contains a US state abbreviation and the cleaned result is "canada",
    try to correct it by expanding the state abbreviation.
    """
    if not original_location or not cleaned_location:
        return cleaned_location
    
    cleaned_lower = cleaned_location.lower()
    
    # If PDL returned "canada" but the original had a US state abbreviation, fix it
    if cleaned_lower in ['canada', 'canadian']:
        # Check if original location has a US state abbreviation
        # Look for US state abbreviations in the original location
        state_pattern = r',\s*([A-Z]{2})(?:\s+\d{5})?$|\s+([A-Z]{2})(?:\s+\d{5})?$'
        match = re.search(state_pattern, original_location, re.IGNORECASE)
        
        if match:
            state_abbr = (match.group(1) or match.group(2)).upper()
            if state_abbr in US_STATE_ABBREVIATIONS:
                # This was likely a US location, not Canada
                # Expand the state abbreviation and return a more appropriate location
                state_full = US_STATE_ABBREVIATIONS[state_abbr]
                # Try to extract city name if present
                city_match = re.match(r'^([^,]+)', original_location)
                if city_match:
                    city = city_match.group(1).strip()
                    return f"{city}, {state_full}"
                else:
                    return state_full
    
    return cleaned_location


def expand_location_state_only(location: str) -> str:
    """
    Expand US state abbreviations in location strings WITHOUT calling PDL API.
    This preserves the original city name and only expands state abbreviations.
    
    Use this for cases where you want to preserve city names and avoid PDL's
    autocorrection/fuzzy matching of city names.
    
    Example: "Research Park, CA" -> "Research Park, California"
    """
    if not location:
        return location
    
    expanded = _expand_us_state_abbreviation(location)
    if expanded != location:
        print(f"Expanded US state abbreviation: '{location}' -> '{expanded}'")
    
    return expanded


def clean_location_name(location, use_pdl_api=True):
    """Clean location name using PDL Cleaner API for better matching
    OPTIMIZED: Uses connection pooling for better performance
    
    FIXED: Properly handles US state abbreviations (e.g., "CA" = California, not Canada)
    
    Args:
        location: Location string to clean
        use_pdl_api: If False, only expands state abbreviations without calling PDL API.
                     This preserves city names and avoids autocorrection.
    """
    if not location:
        return location
    
    # Pre-process: Expand US state abbreviations to avoid confusion with country codes
    # Example: "Research Park, CA" -> "Research Park, California"
    expanded_location = _expand_us_state_abbreviation(location)
    if expanded_location != location:
        print(f"Expanded US state abbreviation: '{location}' -> '{expanded_location}'")
    
    # If PDL API is disabled, just return the expanded location
    if not use_pdl_api:
        return expanded_location
    
    try:
        print(f"Cleaning location: {location}")
        
        # Use session for connection pooling
        with _session_lock:
            response = _session.get(
                f"{PDL_BASE_URL}/location/clean",
                params={
                    'api_key': PEOPLE_DATA_LABS_API_KEY,
                    'location': expanded_location
                },
                timeout=10
            )
        
        if response.status_code == 200:
            clean_data = response.json()
            if clean_data.get('status') == 200 and clean_data.get('name'):
                cleaned_location = clean_data['name']
                
                # Post-process: Fix cases where PDL incorrectly returns "canada" for US locations
                fixed_location = _fix_canada_misinterpretation(location, cleaned_location)
                
                if fixed_location != cleaned_location:
                    print(f"Fixed Canada misinterpretation: '{cleaned_location}' -> '{fixed_location}'")
                
                print(f"Cleaned location: '{location}' -> '{fixed_location}'")
                return fixed_location
    
    except Exception as e:
        print(f"Location cleaning failed: {e}")
    
    return expanded_location


def enrich_job_title_with_pdl(job_title):
    """Use PDL Job Title Enrichment API to get standardized job titles
    OPTIMIZED: Uses connection pooling for better performance
    """
    try:
        print(f"Enriching job title: {job_title}")
        
        # Use session for connection pooling
        with _session_lock:
            response = _session.get(
                f"{PDL_BASE_URL}/job_title/enrich",
                params={
                    'api_key': PEOPLE_DATA_LABS_API_KEY,
                    'job_title': job_title
                },
                timeout=10
            )
        
        if response.status_code == 200:
            enrich_data = response.json()
            if enrich_data.get('status') == 200 and enrich_data.get('data'):
                enriched_data = enrich_data['data']
                
                # Extract useful enrichment data
                result = {
                    'cleaned_name': enriched_data.get('cleaned_name', job_title),
                    'similar_titles': enriched_data.get('similar_job_titles', []),
                    'levels': enriched_data.get('job_title_levels', []),
                    'categories': enriched_data.get('job_title_categories', [])
                }
                
                print(f"Job title enrichment successful: {result}")
                return result
    
    except Exception as e:
        print(f"Job title enrichment failed: {e}")
    
    return {
        'cleaned_name': job_title,
        'similar_titles': [],
        'levels': [],
        'categories': []
    }


def get_autocomplete_suggestions(query, data_type='job_title'):
    """Enhanced autocomplete with proper PDL field mapping"""
    try:
        print(f"Getting autocomplete suggestions for {data_type}: {query}")
        
        # Map your frontend field names to PDL's supported field names
        pdl_field_mapping = {
            'job_title': 'title',  # This is the key fix
            'company': 'company',
            'location': 'location',
            'school': 'school',
            'skill': 'skill',
            'industry': 'industry',
            'role': 'role',
            'sub_role': 'sub_role'
        }
        
        # Get the correct PDL field name
        pdl_field = pdl_field_mapping.get(data_type, data_type)
        
        print(f"Mapping {data_type} -> {pdl_field} for PDL API")
        
        response = requests.get(
            f"{PDL_BASE_URL}/autocomplete",
            params={
                'api_key': PEOPLE_DATA_LABS_API_KEY,
                'field': pdl_field,  # Use the mapped field name
                'text': query,
                'size': 10
            },
            timeout=15
        )
        
        print(f"PDL autocomplete response: {response.status_code}")
        
        if response.status_code == 200:
            auto_data = response.json()
            if auto_data.get('status') == 200 and auto_data.get('data'):
                suggestions = auto_data['data']
                print(f"Autocomplete suggestions: {suggestions}")
                return suggestions
            else:
                print(f"PDL autocomplete no data: {auto_data}")
                return []
        
        elif response.status_code == 400:
            try:
                error_data = response.json()
                print(f"PDL autocomplete error 400: {error_data}")
                if isinstance(error_data, dict) and 'error' in error_data:
                    msg = error_data['error'].get('message', '')
                    if 'Supported fields are' in msg:
                        print(f"Available fields: {msg}")
            except Exception:
                pass
            return []
        elif response.status_code == 402:
            print("PDL API: Payment required for autocomplete")
            return []
        elif response.status_code == 429:
            print("PDL API rate limited for autocomplete")
            return []
        else:
            print(f"PDL autocomplete error {response.status_code}: {response.text}")
            return []
    
    except requests.exceptions.Timeout:
        print(f"Autocomplete timeout for {data_type}: {query}")
        return []
    except Exception as e:
        print(f"Autocomplete exception for {data_type}: {e}")
        return []


def es_title_block(primary_title: str, similar_titles: list[str] | None):
    """Build Elasticsearch-style title block query with validation"""
    titles = [t.strip().lower() for t in ([primary_title] + (similar_titles or [])) if t and t.strip()]
    
    # ‚úÖ CRITICAL: Don't create empty should clauses
    if not titles:
        print("‚ö†Ô∏è WARNING: No valid titles provided, using fallback query")
        # Fallback to a broad professional query
        return {"exists": {"field": "job_title"}}
    
    # PDL doesn't support minimum_should_match at the top level
    # The "should" clause with at least one match is sufficient
    return {
        "bool": {
            "should": (
                [{"match_phrase": {"job_title": t}} for t in titles] +   # exact phrase
                [{"match": {"job_title": t}} for t in titles]            # token match
            )
        }
    }


def es_title_block_from_enrichment(primary_title: str, similar_titles: list[str] | None):
    """Reuse the already-implemented helper"""
    return es_title_block(primary_title, similar_titles or [])


def determine_location_strategy(location_input):
    """Determine whether to use metro or locality search based on input location"""
    try:
        # Handle empty/None location
        if not location_input or not isinstance(location_input, str) or not location_input.strip():
            return {
                'strategy': 'no_location',
                'metro_location': None,
                'city': None,
                'state': None,
                'original_input': '',
                'matched_metro': None
            }
        
        location_lower = location_input.lower().strip()
        
        # Check for country-only searches (United States, USA, US)
        us_aliases = ['united states', 'usa', 'us', 'united states of america']
        if location_lower in us_aliases:
            return {
                'strategy': 'country_only',
                'metro_location': None,
                'city': None,
                'state': None,
                'original_input': location_input,
                'matched_metro': None
            }
        
        # Parse input location
        if ',' in location_lower:
            parts = [part.strip() for part in location_lower.split(',')]
            city = parts[0]
            state = parts[1] if len(parts) > 1 else None
        else:
            city = location_lower
            state = None
        
        # Check if this location maps to a PDL metro area
        metro_key = None
        metro_location = None
        
        # Direct match check
        if city in PDL_METRO_AREAS:
            metro_key = city
            metro_location = PDL_METRO_AREAS[city]
        
        # Also check full location string
        elif location_lower in PDL_METRO_AREAS:
            metro_key = location_lower
            metro_location = PDL_METRO_AREAS[location_lower]
        
        # Check for partial matches (e.g., "san francisco, ca" matches "san francisco")
        else:
            for metro_name in PDL_METRO_AREAS:
                if metro_name in city or city in metro_name:
                    metro_key = metro_name
                    metro_location = PDL_METRO_AREAS[metro_name]
                    break
        
        if metro_location:
            return {
                'strategy': 'metro_primary',
                'metro_location': metro_location,
                'city': city,
                'state': state,
                'original_input': location_input,
                'matched_metro': metro_key
            }
        else:
            return {
                'strategy': 'locality_primary',
                'metro_location': None,
                'city': city,
                'state': state,
                'original_input': location_input,
                'matched_metro': None
            }
            
    except Exception as e:
        print(f"Error determining location strategy: {e}")
        return {
            'strategy': 'locality_primary',
            'metro_location': None,
            'city': location_input,
            'state': None,
            'original_input': location_input,
            'matched_metro': None
        }


def determine_job_level(job_title):
    """Determine job level from job title for JOB_TITLE_LEVELS search"""
    job_title_lower = job_title.lower()
    
    if any(word in job_title_lower for word in ['intern', 'internship']):
        return 'intern'
    elif any(word in job_title_lower for word in ['entry', 'junior', 'associate', 'coordinator']):
        return 'entry'
    elif any(word in job_title_lower for word in ['senior', 'lead', 'principal']):
        return 'senior'
    elif any(word in job_title_lower for word in ['manager', 'director', 'head']):
        return 'manager'
    elif any(word in job_title_lower for word in ['vp', 'vice president', 'executive', 'chief']):
        return 'executive'
    else:
        return 'mid'  # Default to mid-level


def _choose_best_email(emails: list[dict], recommended: str | None = None) -> str | None:
    """Choose the best email from a list of emails"""
    def is_valid(addr: str) -> bool:
        # Handle case where addr might be a boolean or other non-string type
        if not isinstance(addr, str):
            return False
        if not addr or '@' not in addr: 
            return False
        bad = ["example.com", "test.com", "domain.com", "noreply@"]
        return not any(b in addr.lower() for b in bad)
    
    items = []
    for e in emails or []:
        addr = (e.get("address") or "").strip()
        et = (e.get("type") or "").lower()
        if is_valid(addr):
            items.append((et, addr))
    
    for et, a in items:
        if et in ("work","professional"): 
            return a
    for et, a in items:
        if et == "personal": 
            return a
    
    # Handle case where recommended might be a boolean
    if isinstance(recommended, str) and is_valid(recommended): 
        return recommended
        
    return items[0][1] if items else None


def extract_contact_from_pdl_person_enhanced(person, target_company=None, pre_verified_email=None):
    """
    Enhanced contact extraction with relaxed, sensible email acceptance.
    
    Args:
        person: PDL person data dictionary
        target_company: Target company name for email lookup (required for correct domain extraction)
        pre_verified_email: Optional pre-verified email result from batch verification (‚úÖ TASK 2: avoids redundant Hunter calls)
    """
    import time
    extract_start = time.time()
    try:
        print(f"DEBUG: ‚è±Ô∏è  Starting contact extraction")
        
        # Basic identity
        first_name = person.get('first_name', '')
        last_name = person.get('last_name', '')
        if not first_name or not last_name:
            print(f"DEBUG: Missing name")
            return None

        print(f"DEBUG: Name found - {first_name} {last_name}")

        # Experience
        experience = person.get('experience', []) or []
        if not isinstance(experience, list):
            experience = []
            
        work_experience_details, current_job = [], None
        if experience:
            current_job = experience[0]
            for i, job in enumerate(experience[:5]):
                if not isinstance(job, dict):
                    continue
                company_info = job.get('company') or {}
                title_info = job.get('title') or {}
                company_name = company_info.get('name', '') if isinstance(company_info, dict) else ''
                job_title = title_info.get('name', '') if isinstance(title_info, dict) else ''
                start_date = job.get('start_date') or {}
                end_date = job.get('end_date') or {}

                def fmt(d, default_end=False):
                    if not isinstance(d, dict):
                        return ""
                    y = d.get('year')
                    m = d.get('month')
                    if y:
                        return f"{m or 1}/{y}" if m else f"{y}"
                    return "Present" if default_end else ""

                start_str = fmt(start_date)
                end_str = fmt(end_date, default_end=(i == 0))
                if company_name and job_title:
                    duration = f"{start_str} - {end_str}" if start_str else "Date unknown"
                    work_experience_details.append(f"{job_title} at {company_name} ({duration})")

        company_name = ''
        job_title = ''
        if current_job and isinstance(current_job, dict):
            company_info = current_job.get('company') or {}
            title_info = current_job.get('title') or {}
            company_name = company_info.get('name', '') if isinstance(company_info, dict) else ''
            job_title = title_info.get('name', '') if isinstance(title_info, dict) else ''

        # Location
        location_info = person.get('location') or {}
        city = location_info.get('locality', '') if isinstance(location_info, dict) else ''
        state = location_info.get('region', '') if isinstance(location_info, dict) else ''

        # Email selection - VERIFY PDL EMAILS WITH HUNTER USING TARGET COMPANY DOMAIN
        emails = person.get('emails') or []
        if not isinstance(emails, list):
            emails = []
            
        recommended = person.get('recommended_personal_email') or ''
        if not isinstance(recommended, str):
            recommended = ''
            
        pdl_email = _choose_best_email(emails, recommended)
        
        # ‚úÖ USE TARGET COMPANY DOMAIN INSTEAD OF PDL EMAIL DOMAIN
        # Always use target company domain for email lookup (not PDL email domain which may be from old job)
        from app.services.hunter import get_verified_email, get_company_domain
        
        # Get target company domain
        if target_company:
            target_domain = get_company_domain(target_company)
            print(f"[ContactExtraction] Target company: {target_company}")
            print(f"[ContactExtraction] Target company domain: {target_domain}")
        else:
            target_domain = None
            print(f"[ContactExtraction] ‚ö†Ô∏è No target company provided, using PDL company as fallback")
        
        # Get PDL email domain for comparison
        pdl_domain = None
        if pdl_email and "@" in pdl_email:
            pdl_domain = pdl_email.split("@")[1].lower().strip()
            print(f"[ContactExtraction] PDL email domain: {pdl_domain}")
        
        # Log domain comparison
        if target_domain and pdl_domain:
            if pdl_domain != target_domain.lower():
                print(f"[ContactExtraction] ‚ö†Ô∏è PDL email is from old job ({pdl_domain}), using target domain ({target_domain})")
            else:
                print(f"[ContactExtraction] ‚úÖ PDL email matches target domain")
        
        # ‚úÖ TASK 2: Use pre-verified email if available (from batch verification), otherwise verify individually
        if pre_verified_email and pre_verified_email.get('email'):
            # Skip Hunter verification - use batch result
            best_email = pre_verified_email.get('email')
            email_source = pre_verified_email.get('source', 'hunter_batch')
            email_verified = pre_verified_email.get('verified', False)
            print(f"[ContactExtraction] ‚úÖ Using pre-verified email from batch: {best_email} (verified: {email_verified})")
        else:
            # Fall back to individual verification (existing code)
            company_for_email_lookup = target_company if target_company else company_name
            
            # ‚úÖ VERIFY PDL EMAIL WITH HUNTER BEFORE USING IT
            # This ensures we don't use outdated/invalid PDL emails
            # Uses target company domain for correct email lookup
            email_verify_start = time.time()
            verified_email_result = get_verified_email(
                pdl_email=pdl_email if pdl_email and pdl_email != "Not available" else None,
                first_name=first_name,
                last_name=last_name,
                company=company_for_email_lookup,  # Use target company, not PDL person's company
                person_data=person  # Pass full person data for context
            )
            email_verify_time = time.time() - email_verify_start
            print(f"DEBUG: ‚è±Ô∏è  Email verification: {email_verify_time:.2f}s for {first_name} {last_name}")
            
            best_email = verified_email_result.get('email')
            email_source = verified_email_result.get('email_source', 'pdl')
            email_verified = verified_email_result.get('email_verified', False)
        
        # ‚úÖ INCLUDE contacts even without emails (Hunter.io will enrich them)
        if not best_email or best_email == "Not available":
            best_email = "Not available"  # Mark as unavailable but include the contact

        # Phone
        phone_numbers = person.get('phone_numbers') or []
        if not isinstance(phone_numbers, list):
            phone_numbers = []
        phone = phone_numbers[0] if phone_numbers else ''

        # LinkedIn
        profiles = person.get('profiles') or []
        if not isinstance(profiles, list):
            profiles = []
            
        linkedin_url = ''
        for p in profiles:
            if isinstance(p, dict) and 'linkedin' in (p.get('network') or '').lower():
                linkedin_url = p.get('url', '') or ''
                break

        # Education
        education = person.get('education') or []
        if not isinstance(education, list):
            education = []
            
        education_details, college_name = [], ""
        for edu in education:
            if not isinstance(edu, dict):
                continue
            school_info = edu.get('school') or {}
            school_name = school_info.get('name', '') if isinstance(school_info, dict) else ''
            degrees = edu.get('degrees') or []
            
            if not isinstance(degrees, list):
                degrees = []
                
            degree = degrees[0] if degrees else ''
            start_date = edu.get('start_date') or {}
            end_date = edu.get('end_date') or {}
            syear = start_date.get('year') if isinstance(start_date, dict) else None
            eyear = end_date.get('year') if isinstance(end_date, dict) else None

            if school_name:
                entry = school_name
                if degree:
                    entry += f" - {degree}"
                if syear or eyear:
                    entry += f" ({syear or '?'} - {eyear or 'Present'})"
                education_details.append(entry)
                if not college_name and 'high school' not in school_name.lower():
                    college_name = school_name
        education_history = '; '.join(education_details) if education_details else 'Not available'

        # Volunteer
        volunteer_work = []
        interests = person.get('interests') or []
        if not isinstance(interests, list):
            interests = []
            
        for interest in interests:
            if isinstance(interest, str):
                if any(k in interest.lower() for k in ['volunteer', 'charity', 'nonprofit', 'community', 'mentor']):
                    volunteer_work.append(interest)
                elif len(volunteer_work) < 3:
                    volunteer_work.append(f"{interest} enthusiast")

        summary = person.get('summary') or ''
        if isinstance(summary, str):
            vk = ['volunteer', 'charity', 'nonprofit', 'community service', 'mentor', 'coach']
            for k in vk:
                if k in summary.lower():
                    for sentence in summary.split('.'):
                        if k in sentence.lower():
                            volunteer_work.append(sentence.strip())
                            break
        volunteer_history = '; '.join(volunteer_work[:5]) if volunteer_work else 'Not available'

        # Safe email extraction for WorkEmail (use verified email if it's a work email)
        # Use verified email if it's from target company (work email), otherwise use PDL work email
        work_email = 'Not available'
        if best_email and best_email != "Not available":
            # Check if verified email is a work email (from target company domain)
            if target_domain and "@" in best_email:
                email_domain = best_email.split("@")[1].lower().strip()
                if target_domain.lower() in email_domain:
                    work_email = best_email  # Verified work email from target company
                elif email_source == 'hunter.io':
                    work_email = best_email  # Hunter found email (likely work email)
                else:
                    # Personal email - don't use as work email
                    work_email = 'Not available'
            elif email_source == 'hunter.io':
                work_email = best_email  # Hunter found email
            else:
                # Check if it's a work email from PDL
                for e in emails:
                    if isinstance(e, dict):
                        email_addr = e.get('address', '')
                        email_type = (e.get('type') or '').lower()
                        if email_type in ('work', 'professional') and email_addr == best_email:
                            work_email = best_email
                            break
        
        # Extract personal emails for fallback
        personal_emails_list = []
        for e in emails:
            if isinstance(e, dict):
                email_addr = e.get('address', '')
                email_type = (e.get('type') or '').lower()
                if email_type not in ('work', 'professional') and email_addr and "@" in email_addr:
                    personal_emails_list.append(email_addr)
        
        # Add recommended personal email if available
        if recommended and recommended not in personal_emails_list:
            personal_emails_list.append(recommended)

        # Check if currently employed at target company
        is_currently_at_target = False
        if target_company and experience:
            # Check if first job (current job) is at target company and has no end_date
            first_job = experience[0] if isinstance(experience, list) and len(experience) > 0 else None
            if first_job and isinstance(first_job, dict):
                first_job_company = first_job.get('company', {})
                if isinstance(first_job_company, dict):
                    first_job_company_name = first_job_company.get('name', '')
                    # Clean both company names for accurate comparison
                    cleaned_target = clean_company_name(target_company).lower().strip()
                    cleaned_first_job = clean_company_name(first_job_company_name).lower().strip() if first_job_company_name else ''
                    
                    # Check if company matches (exact match after cleaning) AND job has no end_date (indicating current employment)
                    first_job_end_date = first_job.get('end_date')
                    if cleaned_first_job and cleaned_target:
                        # Use exact match or check if cleaned names are very similar (to handle variations like "ASML" vs "ASML Holding")
                        # But be strict - require the core company name to match
                        if cleaned_first_job == cleaned_target or (cleaned_target in cleaned_first_job and len(cleaned_target) >= 3):
                            # If no end_date or end_date is empty, assume current employment
                            if not first_job_end_date or (isinstance(first_job_end_date, dict) and not first_job_end_date.get('year')):
                                is_currently_at_target = True
                                print(f"[ContactExtraction] ‚úÖ Currently at target company ({target_company}) - first job: {first_job_company_name}")
                            else:
                                print(f"[ContactExtraction] ‚ö†Ô∏è Previously at target company ({target_company}), but left (end_date: {first_job_end_date})")
                        else:
                            print(f"[ContactExtraction] ‚ö†Ô∏è Not at target company - first job: {first_job_company_name} (cleaned: {cleaned_first_job}), target: {target_company} (cleaned: {cleaned_target})")

        # Store minimal experience data for anchor detection (first 2 jobs with dates)
        experience_for_anchors = []
        if experience and isinstance(experience, list):
            for i, job in enumerate(experience[:2]):  # Only need first 2 for transition detection
                if isinstance(job, dict):
                    job_data = {
                        'company': job.get('company', {}),
                        'title': job.get('title', {}),
                        'start_date': job.get('start_date', {}),
                        'end_date': job.get('end_date', {})
                    }
                    experience_for_anchors.append(job_data)
        
        contact = {
            'FirstName': first_name,
            'LastName': last_name,
            'LinkedIn': linkedin_url,
            'Email': best_email or "Not available", 
            'Title': job_title,
            'Company': company_name,
            'City': city,
            'State': state,
            'College': college_name,
            'Phone': phone,
            'PersonalEmail': recommended if isinstance(recommended, str) else '',
            'WorkEmail': work_email,
            'SocialProfiles': f'LinkedIn: {linkedin_url}' if linkedin_url else 'Not available',
            'EducationTop': education_history,
            'VolunteerHistory': volunteer_history,
            'WorkSummary': '; '.join(work_experience_details[:3]) if work_experience_details else f"Professional at {company_name}",
            'Group': f"{company_name} {job_title.split()[0] if job_title else 'Professional'} Team",
            'LinkedInConnections': person.get('linkedin_connections', 0),
            'DataVersion': person.get('dataset_version', 'Unknown'),
            'EmailSource': email_source,  # Track email source (pdl or hunter.io)
            'EmailVerified': email_verified,  # Track if email was verified
            'IsCurrentlyAtTarget': is_currently_at_target,  # Track if currently at target company
            'experience': experience_for_anchors  # Store minimal experience for anchor detection
        }

        extract_time = time.time() - extract_start
        print(f"DEBUG: ‚è±Ô∏è  Contact extraction successful: {extract_time:.2f}s total for {first_name} {last_name}")
        return contact
        
    except Exception as e:
        extract_time = time.time() - extract_start
        print(f"DEBUG: ‚è±Ô∏è  Failed to extract enhanced contact ({extract_time:.2f}s): {e}")
        import traceback
        traceback.print_exc()
        return None


def add_pdl_enrichment_fields_optimized(contact, person_data):
    """Add enrichment fields based on your product specifications"""
    try:
        # Work summary using experience array (36.8% fill rate)
        experience = person_data.get('experience', [])
        if isinstance(experience, list) and experience:
            current_job = experience[0]
            if isinstance(current_job, dict):
                title_info = current_job.get('title', {})
                company_info = current_job.get('company', {})
                
                title = title_info.get('name', contact.get('Title', '')) if isinstance(title_info, dict) else contact.get('Title', '')
                company = company_info.get('name', contact.get('Company', '')) if isinstance(company_info, dict) else contact.get('Company', '')
                
                work_summary = f"Current {title} at {company}"
                
                # Add years of experience if available (17.5% fill rate)
                years_exp = person_data.get('inferred_years_experience')
                if years_exp:
                    work_summary += f" ({years_exp} years experience)"
                
                if len(experience) > 1:
                    prev_job = experience[1]
                    if isinstance(prev_job, dict):
                        prev_company_info = prev_job.get('company', {})
                        if isinstance(prev_company_info, dict):
                            prev_company = prev_company_info.get('name', '')
                            if prev_company:
                                work_summary += f". Previously at {prev_company}"
                
                contact['WorkSummary'] = work_summary
        else:
            contact['WorkSummary'] = f"Professional at {contact.get('Company', 'current company')}"
        
        # Volunteer History from interests (4.2% fill rate)
        interests = person_data.get('interests', [])
        if isinstance(interests, list) and interests:
            volunteer_activities = []
            for interest in interests[:3]:  # Top 3 interests
                if isinstance(interest, str):
                    volunteer_activities.append(f"{interest} enthusiast")
            
            contact['VolunteerHistory'] = '; '.join(volunteer_activities) if volunteer_activities else 'Not available'
        else:
            contact['VolunteerHistory'] = 'Not available'
        
        # Group/Department (as per your spec)
        contact['Group'] = f"{contact.get('Company', 'Company')} {contact.get('Title', '').split()[0] if contact.get('Title') else 'Professional'} Team"
        
    except Exception as e:
        print(f"Error adding enrichment fields: {e}")


@retry_with_backoff(
    max_retries=3,
    initial_delay=1.0,
    max_delay=60.0,
    retryable_exceptions=(
        requests.exceptions.RequestException,
        requests.exceptions.Timeout,
        requests.exceptions.ConnectionError,
    ),
)
def execute_pdl_search(headers, url, query_obj, desired_limit, search_type, page_size=50, verbose=False, skip_count=0, target_company=None):
    """
    Execute PDL search with pagination
    
    Args:
        headers: HTTP headers for PDL API
        url: PDL API endpoint URL
        query_obj: PDL query object
        desired_limit: Maximum number of contacts to return
        search_type: Type of search (for logging)
        page_size: Number of results per page
        verbose: Enable verbose logging
        skip_count: Number of results to skip from the beginning (for getting different people)
        target_company: Target company name for email domain extraction (required for correct domain)
    
    Note: Ensures desired_limit and page_size are integers to avoid float slicing errors.
    """
    # ‚úÖ Ensure integer values to avoid float slicing errors
    desired_limit = int(desired_limit)
    page_size = int(page_size)
    skip_count = int(skip_count) if skip_count else 0
    import time
    import random
    
    search_total_start = time.time()
    pdl_api_time = 0.0
    extract_time = 0.0
    
    # Add small random skip to get different results each time
    # Skip between 0-5 results randomly to introduce variation
    if skip_count == 0:
        skip_count = random.randint(0, min(5, desired_limit // 2))
    
    # ---- Page 1
    # Fetch more than needed to account for skipping
    fetch_size = page_size + skip_count if skip_count > 0 else page_size
    # ‚úÖ CRITICAL: Cap at 100 (PDL's max size limit) and ensure integer
    fetch_size = int(min(100, fetch_size))
    body = {"query": query_obj, "size": fetch_size}
    
    # ‚úÖ ADD DEBUG LOGGING
    print(f"\n=== PDL {search_type} DEBUG ===")
    print(f"Query being sent:")
    print(json.dumps(body, indent=2, ensure_ascii=False))
    print("=" * 50)
    
    if verbose:
        print(f"\n=== PDL {search_type} PAGE 1 BODY ===")
        print(json.dumps(body, ensure_ascii=False))

    # Use session for connection pooling
    pdl_api_start = time.time()
    with _session_lock:
        r = _session.post(url, headers=headers, json=body, timeout=30)
    pdl_api_time += time.time() - pdl_api_start
    
    # ‚úÖ HANDLE 404 GRACEFULLY - Return (empty, 404) so prompt-search caller can retry with relaxed query
    if r.status_code == 404:
        print(f"\n‚ùå PDL 404 ERROR - No records found matching query")
        print(f"Response: {r.text}")
        print(f"\nüîç DIAGNOSIS:")
        print(f"   This usually means:")
        print(f"   1. The job title doesn't exist in PDL database")
        print(f"   2. The location filter is too restrictive")
        print(f"   3. The combination of filters yields zero results")
        print(f"\nüí° SUGGESTIONS:")
        print(f"   - Try a broader job title (e.g., 'engineer' instead of 'senior software engineer')")
        print(f"   - Remove the company filter")
        print(f"   - Try a different location or just state instead of city")
        return ([], 404)
    
    # ‚úÖ HANDLE OTHER ERRORS - raise_for_status will handle 4xx/5xx (except 404)
    if r.status_code != 200:
        print(f"\n‚ùå PDL ERROR {r.status_code}:")
        print(f"Response: {r.text[:1000]}")
        r.raise_for_status()  # Raise exception for retry logic to catch
    
    j = r.json()

    data   = j.get("data", []) or []
    total  = j.get("total")
    scroll = j.get("scroll_token")

    if verbose:
        print(f"{search_type} page 1: got {len(data)}; total={total}; scroll_token={scroll}")
    
    # Skip the first skip_count results to get different people
    if skip_count > 0 and len(data) > skip_count:
        data = data[skip_count:]
        if verbose:
            print(f"‚è≠Ô∏è Skipped first {skip_count} results to get different people")

    # Stop early if we already have enough
    if len(data) >= desired_limit or not scroll:
        # OPTIMIZATION: Batch pre-fetch domains for unique companies (when no target_company specified)
        # This reduces duplicate OpenAI calls during extraction
        if not target_company and len(data) > 0:
            batch_domain_start = time.time()
            print(f"[PDL Extract] üîç Pre-fetching domains for unique companies...")
            unique_companies = set()
            for person in data[:desired_limit]:
                experience = person.get('experience', []) or []
                if experience and isinstance(experience, list) and len(experience) > 0:
                    company_info = experience[0].get('company') or {}
                    if isinstance(company_info, dict):
                        company_name = company_info.get('name', '').strip()
                        if company_name and company_name.lower() not in ['', 'n/a', 'none', 'unknown']:
                            unique_companies.add(company_name)
            
            if unique_companies:
                from app.services.hunter import get_smart_company_domain
                # Batch lookup domains in parallel (pre-populate cache)
                with ThreadPoolExecutor(max_workers=min(5, len(unique_companies))) as domain_executor:
                    domain_futures = {
                        domain_executor.submit(get_smart_company_domain, company): company 
                        for company in unique_companies
                    }
                    # Wait for all domain lookups to complete (populates cache)
                    for future in as_completed(domain_futures):
                        company = domain_futures[future]
                        try:
                            domain = future.result()
                            if domain:
                                print(f"[PDL Extract] ‚úÖ Pre-fetched domain: {company} ‚Üí {domain}")
                        except Exception as e:
                            print(f"[PDL Extract] ‚ö†Ô∏è Failed to pre-fetch domain for {company}: {e}")
                batch_domain_time = time.time() - batch_domain_start
                print(f"[PDL Extract] ‚è±Ô∏è  Domain pre-fetch: {batch_domain_time:.2f}s for {len(unique_companies)} companies")
        
        # TRANSFORM THE DATA BEFORE RETURNING - PARALLEL EXTRACTION
        extract_start = time.time()
        extracted_contacts = []
        
        # ‚úÖ ISSUE 1 FIX: Filter duplicates BEFORE extraction to avoid processing same contacts multiple times
        seen_identity_keys = set()
        unique_persons = []
        for person in data[:desired_limit]:
            first_name = (person.get('first_name', '') or '').lower().strip()
            last_name = (person.get('last_name', '') or '').lower().strip()
            # Get company from experience
            company_name = ''
            experience = person.get('experience', []) or []
            if experience and isinstance(experience, list) and len(experience) > 0:
                company_info = experience[0].get('company') or {}
                if isinstance(company_info, dict):
                    company_name = (company_info.get('name', '') or '').lower().strip()
            
            identity_key = f"{first_name}_{last_name}_{company_name}"
            if identity_key not in seen_identity_keys:
                seen_identity_keys.add(identity_key)
                unique_persons.append(person)
        
        duplicate_count = len(data[:desired_limit]) - len(unique_persons)
        if duplicate_count > 0:
            print(f"[PDL Extract] üîç Filtered {duplicate_count} duplicate contacts before extraction")
        
        # ‚úÖ TASK 2: Batch verify emails BEFORE individual extraction to reduce Hunter API calls
        batch_email_results = {}
        person_to_batch_index = {}  # Map unique_persons index -> contacts_for_batch index
        if len(unique_persons) > 1:
            try:
                from app.services.hunter import batch_verify_emails_for_contacts
                
                # Prepare contacts for batch verification (simpler format)
                contacts_for_batch = []
                batch_index = 0
                for person_idx, person in enumerate(unique_persons):
                    first_name = (person.get('first_name', '') or '').strip()
                    last_name = (person.get('last_name', '') or '').strip()
                    if not first_name or not last_name:
                        continue
                    
                    # Get company from experience
                    company_name = ''
                    experience = person.get('experience', []) or []
                    if experience and isinstance(experience, list) and len(experience) > 0:
                        company_info = experience[0].get('company') or {}
                        if isinstance(company_info, dict):
                            company_name = (company_info.get('name', '') or '').strip()
                    
                    # Get PDL email
                    emails = person.get('emails') or []
                    recommended = person.get('recommended_personal_email') or ''
                    pdl_email = _choose_best_email(emails, recommended) if emails or recommended else None
                    
                    contacts_for_batch.append({
                        'first_name': first_name,
                        'last_name': last_name,
                        'company': company_name,
                        'pdl_email': pdl_email if pdl_email and pdl_email != "Not available" else None,
                    })
                    person_to_batch_index[person_idx] = batch_index
                    batch_index += 1
                
                if contacts_for_batch:
                    print(f"[PDL Extract] üìß Batch verifying emails for {len(contacts_for_batch)} contacts...")
                    batch_results = batch_verify_emails_for_contacts(contacts_for_batch, target_company=target_company)
                    # Map batch results back to unique_persons indices
                    for person_idx, batch_idx in person_to_batch_index.items():
                        if batch_idx in batch_results:
                            batch_email_results[person_idx] = batch_results[batch_idx]
                    print(f"[PDL Extract] ‚úÖ Batch verification complete: {len([r for r in batch_email_results.values() if r.get('email')])} emails verified")
            except Exception as batch_error:
                print(f"[PDL Extract] ‚ö†Ô∏è Batch email verification failed, falling back to individual verification: {batch_error}")
                import traceback
                traceback.print_exc()
                batch_email_results = {}
        
        # Use parallel extraction to speed up email verification
        # ‚úÖ ISSUE 5 FIX: Increased from 5 to 10 workers for faster extraction
        max_workers = min(10, len(unique_persons))
        persons_to_extract = unique_persons
        
        if max_workers > 1 and len(persons_to_extract) > 1:
            print(f"[PDL Extract] ‚ö° Parallel extraction with {max_workers} workers for {len(persons_to_extract)} contacts...")
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # Submit all extraction tasks with pre-verified email results
                future_to_person = {
                    executor.submit(
                        extract_contact_from_pdl_person_enhanced, 
                        person, 
                        target_company,
                        batch_email_results.get(i, {})  # Pass pre-verified email result
                    ): (i, person)
                    for i, person in enumerate(persons_to_extract)
                }
                
                # Collect results as they complete, with early stopping for contacts with emails
                contacts_with_emails = 0
                max_contacts_with_emails = desired_limit * 2  # Get up to 2x to have verified options
                for future in as_completed(future_to_person):
                    i, person = future_to_person[future]
                    try:
                        contact = future.result()
                        if contact:  # Only add if extraction was successful
                            extracted_contacts.append(contact)
                            # Check if contact has any email (verified or not)
                            if contact.get('Email') or contact.get('WorkEmail') or contact.get('PersonalEmail'):
                                contacts_with_emails += 1
                            
                            # Early stop if we have enough contacts with emails (verified filtering happens later)
                            if contacts_with_emails >= max_contacts_with_emails:
                                # Cancel remaining futures to save processing time
                                remaining = len(future_to_person) - len(extracted_contacts)
                                if remaining > 0:
                                    print(f"[PDL Extract] ‚ö° Early stopping: Have {contacts_with_emails} contacts with emails, cancelling {remaining} remaining extractions...")
                                    for remaining_future in future_to_person:
                                        if remaining_future != future and not remaining_future.done():
                                            remaining_future.cancel()
                                break
                    except Exception as e:
                        name = f"{person.get('first_name', '')} {person.get('last_name', '')}".strip()
                        print(f"[PDL Extract] ‚ùå Error extracting {name}: {e}")
        else:
            # Fallback to sequential for small batches
            for i, person in enumerate(persons_to_extract):
                try:
                    contact = extract_contact_from_pdl_person_enhanced(person, target_company=target_company)
                    if contact:  # Only add if extraction was successful
                        extracted_contacts.append(contact)
                except Exception as e:
                    name = f"{person.get('first_name', '')} {person.get('last_name', '')}".strip() or f"person {i+1}"
                    print(f"[PDL Extract] ‚ùå Error extracting {name}: {e}")
        
        extract_time = time.time() - extract_start
        total_time = time.time() - search_total_start
        
        print(f"\n[PDL {search_type}] ‚è±Ô∏è  TIMING BREAKDOWN:")
        print(f"[PDL {search_type}]   ‚îú‚îÄ‚îÄ PDL API calls: {pdl_api_time:.2f}s ({pdl_api_time/total_time*100:.1f}%)")
        print(f"[PDL {search_type}]   ‚îî‚îÄ‚îÄ Contact extraction: {extract_time:.2f}s ({extract_time/total_time*100:.1f}%)")
        print(f"[PDL {search_type}] ‚è±Ô∏è  Total: {total_time:.2f}s for {len(extracted_contacts)}/{len(data[:desired_limit])} contacts")
        if len(data[:desired_limit]) > 0:
            avg_extract = extract_time / len(data[:desired_limit])
            print(f"[PDL {search_type}] ‚è±Ô∏è  Avg extraction per contact: {avg_extract:.2f}s")
        
        return (extracted_contacts, 200)

    # ---- Page 2+
    while scroll and len(data) < desired_limit:
        body2 = {"scroll_token": scroll, "size": int(page_size)}
        if verbose:
            print(f"\n=== PDL {search_type} NEXT PAGE BODY ===")
            print(json.dumps(body2, ensure_ascii=False))

        # Use session for connection pooling
        pdl_api_start = time.time()
        with _session_lock:
            r2 = _session.post(url, headers=headers, json=body2, timeout=30)
        pdl_api_time += time.time() - pdl_api_start

        # Be robust to cluster quirk: require query/sql
        if r2.status_code == 400 and "Either `query` or `sql` must be provided" in (r2.text or ""):
            if verbose:
                print(f"{search_type} retrying with query+scroll_token due to 400‚Ä¶")
            body2_fallback = {"query": query_obj, "scroll_token": scroll, "size": int(page_size)}
            pdl_api_start = time.time()
            with _session_lock:
                r2 = _session.post(url, headers=headers, json=body2_fallback, timeout=30)
            pdl_api_time += time.time() - pdl_api_start

        if r2.status_code != 200:
            if verbose:
                print(f"{search_type} next page status={r2.status_code} err={r2.text}")
            break

        j2 = r2.json()
        batch  = j2.get("data", []) or []
        scroll = j2.get("scroll_token")
        data.extend(batch)

        if verbose:
            print(f"{search_type} next page: got {len(batch)}, total so far={len(data)}, next scroll={scroll}")

    # OPTIMIZATION: Batch pre-fetch domains for unique companies (when no target_company specified)
    # This reduces duplicate OpenAI calls during extraction
    if not target_company and len(data) > 0:
        batch_domain_start = time.time()
        print(f"[PDL Extract] üîç Pre-fetching domains for unique companies...")
        unique_companies = set()
        for person in data[:desired_limit]:
            experience = person.get('experience', []) or []
            if experience and isinstance(experience, list) and len(experience) > 0:
                company_info = experience[0].get('company') or {}
                if isinstance(company_info, dict):
                    company_name = company_info.get('name', '').strip()
                    if company_name and company_name.lower() not in ['', 'n/a', 'none', 'unknown']:
                        unique_companies.add(company_name)
        
        if unique_companies:
            from app.services.hunter import get_smart_company_domain
            # Batch lookup domains in parallel (pre-populate cache)
            with ThreadPoolExecutor(max_workers=min(5, len(unique_companies))) as domain_executor:
                domain_futures = {
                    domain_executor.submit(get_smart_company_domain, company): company 
                    for company in unique_companies
                }
                # Wait for all domain lookups to complete (populates cache)
                for future in as_completed(domain_futures):
                    company = domain_futures[future]
                    try:
                        domain = future.result()
                        if domain:
                            print(f"[PDL Extract] ‚úÖ Pre-fetched domain: {company} ‚Üí {domain}")
                    except Exception as e:
                        print(f"[PDL Extract] ‚ö†Ô∏è Failed to pre-fetch domain for {company}: {e}")
            batch_domain_time = time.time() - batch_domain_start
            print(f"[PDL Extract] ‚è±Ô∏è  Domain pre-fetch: {batch_domain_time:.2f}s for {len(unique_companies)} companies")

    # TRANSFORM ALL THE DATA BEFORE RETURNING - PARALLEL EXTRACTION
    extract_start = time.time()
    extracted_contacts = []
    
    # ‚úÖ ISSUE 1 FIX: Filter duplicates BEFORE extraction to avoid processing same contacts multiple times
    seen_identity_keys = set()
    unique_persons = []
    for person in data[:desired_limit]:
        first_name = (person.get('first_name', '') or '').lower().strip()
        last_name = (person.get('last_name', '') or '').lower().strip()
        # Get company from experience
        company_name = ''
        experience = person.get('experience', []) or []
        if experience and isinstance(experience, list) and len(experience) > 0:
            company_info = experience[0].get('company') or {}
            if isinstance(company_info, dict):
                company_name = (company_info.get('name', '') or '').lower().strip()
        
        identity_key = f"{first_name}_{last_name}_{company_name}"
        if identity_key not in seen_identity_keys:
            seen_identity_keys.add(identity_key)
            unique_persons.append(person)
    
    duplicate_count = len(data[:desired_limit]) - len(unique_persons)
    if duplicate_count > 0:
        print(f"[PDL Extract] üîç Filtered {duplicate_count} duplicate contacts before extraction")
    
    # ‚úÖ TASK 2: Batch verify emails BEFORE individual extraction to reduce Hunter API calls
    batch_email_results = {}
    person_to_batch_index = {}  # Map unique_persons index -> contacts_for_batch index
    if len(unique_persons) > 1:
        try:
            from app.services.hunter import batch_verify_emails_for_contacts
            
            # Prepare contacts for batch verification (simpler format)
            contacts_for_batch = []
            batch_index = 0
            for person_idx, person in enumerate(unique_persons):
                first_name = (person.get('first_name', '') or '').strip()
                last_name = (person.get('last_name', '') or '').strip()
                if not first_name or not last_name:
                    continue
                
                # Get company from experience
                company_name = ''
                experience = person.get('experience', []) or []
                if experience and isinstance(experience, list) and len(experience) > 0:
                    company_info = experience[0].get('company') or {}
                    if isinstance(company_info, dict):
                        company_name = (company_info.get('name', '') or '').strip()
                
                # Get PDL email
                emails = person.get('emails') or []
                recommended = person.get('recommended_personal_email') or ''
                pdl_email = _choose_best_email(emails, recommended) if emails or recommended else None
                
                contacts_for_batch.append({
                    'first_name': first_name,
                    'last_name': last_name,
                    'company': company_name,
                    'pdl_email': pdl_email if pdl_email and pdl_email != "Not available" else None,
                })
                person_to_batch_index[person_idx] = batch_index
                batch_index += 1
            
            if contacts_for_batch:
                print(f"[PDL Extract] üìß Batch verifying emails for {len(contacts_for_batch)} contacts...")
                batch_results = batch_verify_emails_for_contacts(contacts_for_batch, target_company=target_company)
                # Map batch results back to unique_persons indices
                for person_idx, batch_idx in person_to_batch_index.items():
                    if batch_idx in batch_results:
                        batch_email_results[person_idx] = batch_results[batch_idx]
                print(f"[PDL Extract] ‚úÖ Batch verification complete: {len([r for r in batch_email_results.values() if r.get('email')])} emails verified")
        except Exception as batch_error:
            print(f"[PDL Extract] ‚ö†Ô∏è Batch email verification failed, falling back to individual verification: {batch_error}")
            import traceback
            traceback.print_exc()
            batch_email_results = {}
    
    # Use parallel extraction to speed up email verification
    # ‚úÖ ISSUE 5 FIX: Increased from 5 to 10 workers for faster extraction
    max_workers = min(10, len(unique_persons))
    persons_to_extract = unique_persons
    
    if max_workers > 1 and len(persons_to_extract) > 1:
        print(f"[PDL Extract] ‚ö° Parallel extraction with {max_workers} workers for {len(persons_to_extract)} contacts...")
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all extraction tasks with pre-verified email results
            future_to_person = {
                executor.submit(
                    extract_contact_from_pdl_person_enhanced, 
                    person, 
                    target_company,
                    batch_email_results.get(i, {})  # Pass pre-verified email result
                ): (i, person)
                for i, person in enumerate(persons_to_extract)
            }
            
            # Collect results as they complete, with early stopping for contacts with emails
            contacts_with_emails = 0
            max_contacts_with_emails = desired_limit * 2  # Get up to 2x to have verified options
            for future in as_completed(future_to_person):
                i, person = future_to_person[future]
                try:
                    contact = future.result()
                    if contact:  # Only add if extraction was successful
                        extracted_contacts.append(contact)
                        # Check if contact has any email (verified or not)
                        if contact.get('Email') or contact.get('WorkEmail') or contact.get('PersonalEmail'):
                            contacts_with_emails += 1
                        
                        # Early stop if we have enough contacts with emails (verified filtering happens later)
                        if contacts_with_emails >= max_contacts_with_emails:
                            # Cancel remaining futures to save processing time
                            remaining = len(future_to_person) - len(extracted_contacts)
                            if remaining > 0:
                                print(f"[PDL Extract] ‚ö° Early stopping: Have {contacts_with_emails} contacts with emails, cancelling {remaining} remaining extractions...")
                                for remaining_future in future_to_person:
                                    if remaining_future != future and not remaining_future.done():
                                        remaining_future.cancel()
                            break
                except Exception as e:
                    name = f"{person.get('first_name', '')} {person.get('last_name', '')}".strip()
                    print(f"[PDL Extract] ‚ùå Error extracting {name}: {e}")
    else:
        # Fallback to sequential for small batches
        for i, person in enumerate(persons_to_extract):
            try:
                # Pass pre-verified email result if available
                pre_verified = batch_email_results.get(i, {})
                contact = extract_contact_from_pdl_person_enhanced(person, target_company=target_company, pre_verified_email=pre_verified)
                if contact:  # Only add if extraction was successful
                    extracted_contacts.append(contact)
            except Exception as e:
                name = f"{person.get('first_name', '')} {person.get('last_name', '')}".strip() or f"person {i+1}"
                print(f"[PDL Extract] ‚ùå Error extracting {name}: {e}")
    
    extract_time = time.time() - extract_start
    total_time = time.time() - search_total_start
    
    print(f"\n[PDL {search_type}] ‚è±Ô∏è  TIMING BREAKDOWN:")
    print(f"[PDL {search_type}]   ‚îú‚îÄ‚îÄ PDL API calls: {pdl_api_time:.2f}s ({pdl_api_time/total_time*100:.1f}%)")
    print(f"[PDL {search_type}]   ‚îî‚îÄ‚îÄ Contact extraction: {extract_time:.2f}s ({extract_time/total_time*100:.1f}%)")
    print(f"[PDL {search_type}] ‚è±Ô∏è  Total: {total_time:.2f}s for {len(extracted_contacts)}/{len(data[:desired_limit])} contacts")
    if len(data[:desired_limit]) > 0:
        avg_extract = extract_time / len(data[:desired_limit])
        print(f"[PDL {search_type}] ‚è±Ô∏è  Avg extraction per contact: {avg_extract:.2f}s")
    
    return (extracted_contacts, 200)


def try_metro_search_optimized(clean_title, similar_titles, company, location_strategy, max_contacts=8, college_alumni=None, exclude_keys=None):
    """Metro search with complete validation and exclusion filtering"""
    
    # ‚úÖ Handle exclusion keys
    excluded_keys = exclude_keys or set()
    
    # Validate inputs
    if not clean_title or not clean_title.strip():
        print("‚ùå No valid job title")
        return []
    
    strategy = location_strategy.get("strategy", "locality_primary")
    metro_location = (location_strategy.get("metro_location") or "").lower()
    city = (location_strategy.get("city") or "").lower()
    state = (location_strategy.get("state") or "").lower()
    
    # Handle country-only search (United States, USA, US)
    if strategy == 'country_only':
        # Build title block with validation
        title_block = es_title_block_from_enrichment(clean_title, similar_titles)
        
        # Validate title block isn't empty
        if not title_block.get("bool", {}).get("should") and not title_block.get("exists"):
            print("‚ùå Empty title block")
            return []
        
        # Only filter by country for country-only searches
        location_must = [{"term": {"location_country": "united states"}}]
        loc_block = {"bool": {"must": location_must}}
    elif not metro_location and not city:
        print("‚ùå No valid location")
        return []
    else:
        # Build title block with validation
        title_block = es_title_block_from_enrichment(clean_title, similar_titles)
        
        # Validate title block isn't empty
        if not title_block.get("bool", {}).get("should") and not title_block.get("exists"):
            print("‚ùå Empty title block")
            return []
        
        # Build location filter
        location_must = []
        
        # Use match instead of term for more flexible location matching
        if metro_location and city:
            location_must.append({
                "bool": {
                    "should": [
                        {"match": {"location_metro": metro_location}},
                        {"match": {"location_locality": city}}
                    ]
                }
            })
        elif metro_location:
            location_must.append({"match": {"location_metro": metro_location}})
        elif city:
            location_must.append({"match": {"location_locality": city}})
        
        # Make state optional - use should instead of must
        if state:
            location_must.append({
                "bool": {
                    "should": [
                        {"match": {"location_region": state}},
                        {"match": {"location_locality": state}}  # Sometimes state is in city field
                    ]
                }
            })
        
        location_must.append({"term": {"location_country": "united states"}})
        
        loc_block = {"bool": {"must": location_must}}

    # Build final query - only include blocks that exist
    must = [title_block]
    if loc_block:
        must.append(loc_block)
    
    # Add optional filters - use both match_phrase and match for flexibility
    if company and company.strip():
        company_lower = company.lower().strip()
        must.append({
            "bool": {
                "should": [
                    {"match_phrase": {"job_company_name": company_lower}},
                    {"match": {"job_company_name": company_lower}}
                ]
            }
        })
    
    # ‚úÖ ADD EDUCATION FILTER TO QUERY - use both match_phrase and match
    if college_alumni:
        aliases = _school_aliases(college_alumni)
        if aliases:
            # Use both match_phrase (exact) and match (flexible) for better coverage
            education_clauses = []
            for a in aliases:
                education_clauses.append({"match_phrase": {"education.school.name": a}})
                education_clauses.append({"match": {"education.school.name": a}})
            must.append({
                "bool": {
                    "should": education_clauses
                }
            })

    # ‚úÖ ALL FILTERS IN MUST CLAUSE - PDL only returns people matching ALL criteria:
    #   1. Job title (title_block)
    #   2. Location (loc_block) 
    #   3. Company (if provided)
    #   4. Education/school (if provided)
    # This is efficient - PDL filters at query time, not post-processing
    query_obj = {"bool": {"must": must}}

    # Execute with proper error handling
    try:
        PDL_URL = f"{PDL_BASE_URL}/person/search"
        headers = {
            "Accept": "application/json",
            "Content-Type": "application/json",
            "X-Api-Key": PEOPLE_DATA_LABS_API_KEY,
        }

        # ‚úÖ Use max_contacts directly - caller already applies 2.5x multiplier
        # Only add small buffer for post-filtering (alumni filtering, exclusion keys)
        if college_alumni:
            # Alumni filter in query means PDL returns mostly alumni
            fetch_limit = max_contacts * 1.5  # Small buffer for variation
        else:
            # No alumni filter - minimal buffer for exclusion keys
            fetch_limit = max_contacts * 1.5  # Minimal buffer for exclusion keys
        
        # Cap fetch_limit to prevent over-fetching (max 2.5x or 50, whichever is smaller)
        fetch_limit = int(min(fetch_limit, max_contacts * 2.5, 50))
        
        page_size = int(min(100, max(1, fetch_limit)))
        
        raw_contacts, _ = execute_pdl_search(
            headers=headers,
            url=PDL_URL,
            query_obj=query_obj,
            desired_limit=int(fetch_limit),
            search_type=f"metro_{location_strategy.get('matched_metro','unknown')}",
            page_size=page_size,
            verbose=False,
            target_company=company  # Pass target company for correct domain extraction
        )
        
        # ‚úÖ FILTER OUT EXCLUDED CONTACTS
        if excluded_keys:
            filtered_contacts = []
            skipped_count = 0
            
            for contact in raw_contacts:
                contact_key = get_contact_identity(contact)
                if contact_key in excluded_keys:
                    skipped_count += 1
                    continue
                filtered_contacts.append(contact)
                if len(filtered_contacts) >= max_contacts:
                    break
            
            print(f"üîç Metro search filtering:")
            print(f"   - Raw results from PDL: {len(raw_contacts)}")
            print(f"   - Excluded (already seen): {skipped_count}")
            print(f"   - Unique new contacts: {len(filtered_contacts)}")
            
            return filtered_contacts[:max_contacts]
        else:
            return raw_contacts[:max_contacts]
            
    except Exception as e:
        print(f"Metro search failed: {e}")
        return []

def try_locality_search_optimized(clean_title, similar_titles, company, location_strategy, max_contacts=8, college_alumni=None, exclude_keys=None):
    """
    Locality-focused version (used when metro results are thin).
    STRICT LOCATION: city AND state AND country, or country-only for United States
    """
    # ‚úÖ Handle exclusion keys
    excluded_keys = exclude_keys or set()
    
    title_block = es_title_block_from_enrichment(clean_title, similar_titles)
    
    strategy = location_strategy.get("strategy", "locality_primary")
    
    # Handle country-only search (United States, USA, US)
    if strategy == 'country_only':
        # Only filter by country for country-only searches
        location_must = [{"term": {"location_country": "united states"}}]
        loc_block = {"bool": {"must": location_must}}
    else:
        # BUILD STRICT LOCATION FILTER
        city = (location_strategy.get("city") or "").lower()
        state = (location_strategy.get("state") or "").lower()
        
        location_must = []
        
        # Use match for more flexible city matching
        if city:
            location_must.append({"match": {"location_locality": city}})
        
        # Make state optional - use should for flexibility
        if state:
            location_must.append({
                "bool": {
                    "should": [
                        {"match": {"location_region": state}},
                        {"match": {"location_locality": state}}  # Sometimes state is in city field
                    ]
                }
            })
        
        # Always require USA
        location_must.append({"term": {"location_country": "united states"}})
        
        loc_block = {"bool": {"must": location_must}}

    must = [title_block, loc_block]
    if company:
        # Use both match_phrase and match for flexibility
        company_lower = company.lower().strip()
        must.append({
            "bool": {
                "should": [
                    {"match_phrase": {"job_company_name": company_lower}},
                    {"match": {"job_company_name": company_lower}}
                ]
            }
        })
    
    # ‚úÖ NO EDUCATION FILTER IN QUERY - alumni filtering happens post-fetch via _contact_has_school_as_primary_education_lenient()
    # This makes the PDL query broader (title + company + location) and more likely to return results
    # Python post-filtering then verifies alumni status

    # ‚úÖ ALL FILTERS IN MUST CLAUSE - PDL only returns people matching ALL criteria:
    #   1. Job title (title_block)
    #   2. Location (loc_block) 
    #   3. Company (if provided)
    # Education/school filtering happens post-fetch in Python
    query_obj = {"bool": {"must": must}}

    PDL_URL = f"{PDL_BASE_URL}/person/search"
    headers = {
        "Accept": "application/json",
        "Content-Type": "application/json",
        "X-Api-Key": PEOPLE_DATA_LABS_API_KEY,
    }

    # ‚úÖ Use max_contacts directly - caller already applies 2.5x multiplier
    # Only add small buffer for post-filtering (alumni filtering, exclusion keys)
    if college_alumni:
        # Post-filtering means we need a small buffer
        fetch_limit = max_contacts * 2  # Small buffer for post-filtering
    else:
        fetch_limit = max_contacts * 1.5  # Minimal buffer for exclusion keys
    
    # Cap fetch_limit to prevent over-fetching (max 2.5x or 50, whichever is smaller)
    fetch_limit = int(min(fetch_limit, max_contacts * 2.5, 50))
    
    page_size = int(min(100, max(1, fetch_limit)))

    raw_contacts, _ = execute_pdl_search(
        headers=headers,
        url=PDL_URL,
        query_obj=query_obj,
        desired_limit=int(fetch_limit),
        search_type=f"locality_{location_strategy.get('city','unknown')}",
        page_size=page_size,
        verbose=False,
        target_company=company  # Pass target company for correct domain extraction
    )
    
    # ‚úÖ FILTER OUT EXCLUDED CONTACTS
    if excluded_keys:
        filtered_contacts = []
        skipped_count = 0
        
        for contact in raw_contacts:
            contact_key = get_contact_identity(contact)
            if contact_key in excluded_keys:
                skipped_count += 1
                continue
            filtered_contacts.append(contact)
            if len(filtered_contacts) >= max_contacts:
                break
        
        print(f"üîç Locality search filtering:")
        print(f"   - Raw results from PDL: {len(raw_contacts)}")
        print(f"   - Excluded (already seen): {skipped_count}")
        print(f"   - Unique new contacts: {len(filtered_contacts)}")
        
        return filtered_contacts[:max_contacts]
    else:
        return raw_contacts[:max_contacts]


def try_job_title_levels_search_enhanced(job_title_enrichment, company, city, state, max_contacts, college_alumni=None, exclude_keys=None):
    """Enhanced job title levels search"""
    # ‚úÖ Handle exclusion keys
    excluded_keys = exclude_keys or set()
    
    print("Enhanced job title levels search")

    must = []

    levels = job_title_enrichment.get('levels') or []
    if levels:
        must.append({"bool": {"should": [{"match": {"job_title_levels": lvl}} for lvl in levels]}})
    else:
        jl = determine_job_level(job_title_enrichment.get('cleaned_name', ''))
        if jl:
            must.append({"match": {"job_title_levels": jl}})

    # Also broaden titles
    must.append(es_title_block(job_title_enrichment.get('cleaned_name',''),
                               job_title_enrichment.get('similar_titles') or []))

    if company:
        # Use both match_phrase and match for flexibility
        company_lower = (company or "").lower().strip()
        must.append({
            "bool": {
                "should": [
                    {"match_phrase": {"job_company_name": company_lower}},
                    {"match": {"job_company_name": company_lower}}
                ]
            }
        })

    location_must = []

    # Use match for more flexible city matching
    if city:
        location_must.append({"match": {"location_locality": city}})

    # Make state optional - use should for flexibility
    if state:
        location_must.append({
            "bool": {
                "should": [
                    {"match": {"location_region": state}},
                    {"match": {"location_locality": state}}  # Sometimes state is in city field
                ]
            }
        })

    # Always require USA
    location_must.append({"term": {"location_country": "united states"}})

    must.append({"bool": {"must": location_must}})

    # ‚úÖ NO EDUCATION FILTER IN QUERY - alumni filtering happens post-fetch via _contact_has_school_as_primary_education_lenient()
    # This makes the PDL query broader (title + company + location) and more likely to return results
    # Python post-filtering then verifies alumni status

    # ‚úÖ ALL FILTERS IN MUST CLAUSE - PDL only returns people matching ALL criteria:
    #   1. Job title (title_block)
    #   2. Location (loc_block) 
    #   3. Company (if provided)
    # Education/school filtering happens post-fetch in Python
    query_obj = {"bool": {"must": must}}

    PDL_URL = f"{PDL_BASE_URL}/person/search"
    headers = {
        "Accept": "application/json",
        "Content-Type": "application/json",
        "X-Api-Key": PEOPLE_DATA_LABS_API_KEY,
    }

    # ‚úÖ Use max_contacts directly - caller already applies 2.5x multiplier
    # Only add small buffer for post-filtering (alumni filtering, exclusion keys)
    if college_alumni:
        # Post-filtering means we need a small buffer
        fetch_limit = max_contacts * 2  # Small buffer for post-filtering
    else:
        fetch_limit = max_contacts * 1.5  # Minimal buffer for exclusion keys
    
    # Cap fetch_limit to prevent over-fetching (max 2.5x or 50, whichever is smaller)
    fetch_limit = int(min(fetch_limit, max_contacts * 2.5, 50))
    
    page_size = int(min(100, max(1, fetch_limit)))

    raw_contacts, _ = execute_pdl_search(
        headers=headers,
        url=PDL_URL,
        query_obj=query_obj,
        desired_limit=int(fetch_limit),
        search_type="job_levels_enhanced",
        page_size=page_size,
        verbose=False,
        target_company=company  # Pass target company for correct domain extraction
    )
    
    # ‚úÖ FILTER OUT EXCLUDED CONTACTS
    if excluded_keys:
        filtered_contacts = []
        skipped_count = 0
        
        for contact in raw_contacts:
            contact_key = get_contact_identity(contact)
            if contact_key in excluded_keys:
                skipped_count += 1
                continue
            filtered_contacts.append(contact)
            if len(filtered_contacts) >= max_contacts:
                break
        
        print(f"üîç Job levels search filtering:")
        print(f"   - Raw results from PDL: {len(raw_contacts)}")
        print(f"   - Excluded (already seen): {skipped_count}")
        print(f"   - Unique new contacts: {len(filtered_contacts)}")
        
        return filtered_contacts[:max_contacts]
    else:
        return raw_contacts[:max_contacts]


def search_contacts_with_smart_location_strategy(
    job_title, company, location, max_contacts=8, college_alumni=None, exclude_keys=None
):

    """
    ENHANCED VERSION: Guarantees the requested number of verified alumni contacts
    
    When alumni filter is active, this function will:
    1. Fetch contacts in batches
    2. Filter for verified degree holders
    3. Continue fetching until we have enough verified alumni
    4. Return exactly the requested number
    
    PERFORMANCE OPTIMIZED: Parallelizes enrichment/cleaning operations
    """
    try:
        # ‚úÖ ADD COMPREHENSIVE INPUT VALIDATION
        print(f"\n{'='*70}")
        print(f"üîç PDL SEARCH STARTED (OPTIMIZED)")
        print(f"{'='*70}")
        print(f"üì• Input Parameters:")
        print(f"  ‚îú‚îÄ job_title: '{job_title}'")
        print(f"  ‚îú‚îÄ company: '{company}'")
        print(f"  ‚îú‚îÄ location: '{location}'")
        print(f"  ‚îú‚îÄ max_contacts: {max_contacts}")
        print(f"  ‚îú‚îÄ college_alumni: '{college_alumni}'")
        print(f"  ‚îî‚îÄ exclude_keys: {len(exclude_keys) if exclude_keys else 0} contacts")
        
        # ‚úÖ VALIDATE REQUIRED INPUTS
        # Note: job_title can be empty for prompt search (will search without job title filter)
        # if not job_title or not job_title.strip():
        #     print(f"‚ùå ERROR: job_title is required but was empty or None")
        #     return []
        
        # Location is required for general search (not prompt search)
        if not location or not location.strip():
            print(f"‚ùå ERROR: location is required but was empty or None")
            return []
        
        print(f"{'='*70}\n")
        
        print(f"Starting smart location search for {job_title} at {company} in {location}")
        if college_alumni:
            print(f"üéì Alumni filter enabled: {college_alumni}")
        
        # ‚úÖ OPTIMIZED: Parallelize enrichment and cleaning operations
        import time
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=3) as executor:
            # Submit all enrichment/cleaning tasks in parallel
            future_title = executor.submit(cached_enrich_job_title, job_title)
            future_company = executor.submit(clean_company_name, company) if company else None
            future_location = executor.submit(clean_location_name, location)
            
            # Wait for all to complete
            job_title_enrichment = future_title.result()
            cleaned_company = future_company.result() if future_company else ''
            cleaned_location = future_location.result()
        
        enrichment_time = time.time() - start_time
        print(f"‚ö° Parallel enrichment completed in {enrichment_time:.2f}s (vs ~3-5s sequential)")
        
        primary_title = job_title_enrichment.get('cleaned_name', job_title).lower()
        similar_titles = [t.lower() for t in job_title_enrichment.get('similar_titles', [])[:4]]
        
        # Analyze location strategy
        location_strategy = determine_location_strategy(cleaned_location)
        
        print(f"Location strategy: {location_strategy['strategy']}")
        if location_strategy['matched_metro']:
            print(f"Matched metro: {location_strategy['matched_metro']} -> {location_strategy['metro_location']}")
        
        # ‚úÖ FIX #6: Pre-populate domain cache before parallel searches
        # This ensures both metro and locality searches share the same cache
        if cleaned_company:
            from app.services.hunter import get_smart_company_domain
            print(f"üîç Pre-populating domain cache for company: {cleaned_company}")
            domain = get_smart_company_domain(cleaned_company)
            if domain:
                print(f"‚úÖ Domain cached: {cleaned_company} ‚Üí {domain}")
        
        # Step 4: CHANGED - If alumni filter is active, use batch fetching strategy
        if college_alumni:
            return _fetch_verified_alumni_contacts(
                primary_title, similar_titles, cleaned_company,
                location_strategy, job_title_enrichment,
                max_contacts, college_alumni, exclude_keys
            )
        
        # Step 5: For non-alumni searches, use optimized parallel search
        search_start = time.time()
        contacts = _fetch_contacts_standard_parallel(
            primary_title, similar_titles, cleaned_company,
            location_strategy, job_title_enrichment,
            max_contacts, exclude_keys
        )
        search_time = time.time() - search_start
        
        total_time = time.time() - start_time
        
        # LOG FINAL RESULTS WITH TIMING
        print(f"\n{'='*70}")
        print(f"‚è±Ô∏è  SEARCH TIMING SUMMARY")
        print(f"{'='*70}")
        print(f"‚è±Ô∏è  Enrichment: {enrichment_time:.2f}s ({enrichment_time/total_time*100:.1f}%)")
        print(f"‚è±Ô∏è  PDL search + extraction: {search_time:.2f}s ({search_time/total_time*100:.1f}%)")
        print(f"‚è±Ô∏è  Total time: {total_time:.2f}s")
        print(f"üìä Contacts found: {len(contacts)}/{max_contacts}")
        if len(contacts) > 0:
            avg_time_per_contact = search_time / len(contacts)
            print(f"‚è±Ô∏è  Avg time per contact: {avg_time_per_contact:.2f}s")
        
        if len(contacts) == 0:
            print(f"‚ö†Ô∏è  WARNING: No contacts found with valid emails for {job_title} in {location}")
            print(f"Search parameters: title='{primary_title}', company='{cleaned_company}', location='{cleaned_location}'")
        else:
            print(f"‚úÖ Smart location search completed: {len(contacts)} contacts found with valid emails")
        print(f"{'='*70}\n")
        
        return contacts[:max_contacts]
        
    except Exception as e:
        print(f"Smart location search failed: {e}")
        import traceback
        traceback.print_exc()
        return []



def search_contacts_with_pdl_optimized(job_title, company, location, max_contacts=8):
    """Updated main search function using smart location strategy"""
    return search_contacts_with_smart_location_strategy(job_title, company, location, max_contacts)


def search_contacts_with_pdl(job_title, company, location, max_contacts=8):
    """Wrapper function - redirect to optimized version for backward compatibility"""
    return search_contacts_with_pdl_optimized(job_title, company, location, max_contacts)


def build_query_from_prompt(parsed_prompt: dict, retry_level: int = 0) -> dict:
    """
    Build PDL Elasticsearch bool query from structured prompt parser output.
    Uses same patterns as es_title_block, try_metro_search_optimized (location, company, schools).
    Returns query_obj ready for execute_pdl_search.

    retry_level: 0=full query; 1=simplify title to single broad match on core role; 2=drop title; 3=drop title and location.
    Company and school filters are never dropped (user's core intent).
    """
    must = []

    # ---- Title block ----
    title_variations = parsed_prompt.get("title_variations") or []
    titles = [t.strip().lower() for t in title_variations if t and str(t).strip()]

    if retry_level == 0:
        # Full: all title variations (phrase + match)
        if titles:
            title_clauses = (
                [{"match_phrase": {"job_title": t}} for t in titles]
                + [{"match": {"job_title": t}} for t in titles]
            )
            title_block = {"bool": {"should": title_clauses}}
            must.append(title_block)
        else:
            must.append({"exists": {"field": "job_title"}})
    elif retry_level == 1:
        # Retry 1: single broad match on core role (first title only)
        if titles:
            core_role = titles[0]
            title_block = {"match": {"job_title": core_role}}
            must.append(title_block)
        else:
            must.append({"exists": {"field": "job_title"}})
    elif retry_level == 2:
        # Retry 2: no title filter (any role at company + school)
        pass
    else:
        # Retry 3: no title, no location (handled below)
        pass

    # ---- Location block (skip when retry_level >= 3) ----
    if retry_level < 3:
        locations = parsed_prompt.get("locations") or []
        location_str = (locations[0] if locations else "").strip() if locations else ""
        if not location_str:
            location_str = "united states"
        cleaned_location = clean_location_name(location_str, use_pdl_api=False)
        location_strategy = determine_location_strategy(cleaned_location)
        strategy = location_strategy.get("strategy", "locality_primary")
        metro_location = (location_strategy.get("metro_location") or "").lower()
        city = (location_strategy.get("city") or "").lower()
        state = (location_strategy.get("state") or "").lower()

        if strategy == "country_only":
            location_must = [{"term": {"location_country": "united states"}}]
        else:
            location_must = []
            if metro_location and city:
                location_must.append({
                    "bool": {
                        "should": [
                            {"match": {"location_metro": metro_location}},
                            {"match": {"location_locality": city}},
                        ]
                    }
                })
            elif metro_location:
                location_must.append({"match": {"location_metro": metro_location}})
            elif city:
                location_must.append({"match": {"location_locality": city}})
            if state:
                location_must.append({
                    "bool": {
                        "should": [
                            {"match": {"location_region": state}},
                            {"match": {"location_locality": state}},
                        ]
                    }
                })
            location_must.append({"term": {"location_country": "united states"}})
        loc_block = {"bool": {"must": location_must}}
        must.append(loc_block)

    # ---- Company block: never dropped ----
    # Use both match_phrase (exact) and match (tokens) so e.g. "bain" matches "Bain & Company", "Bain Capital". Post-validation filters false positives.
    companies = parsed_prompt.get("companies") or []
    company_names = [c.get("name", "").strip() for c in companies if isinstance(c, dict) and c.get("name")]
    if company_names:
        company_clauses = []
        for name in company_names:
            n = name.lower().strip()
            if n:
                # Per company: match_phrase OR match for flexibility
                company_clauses.append({
                    "bool": {
                        "should": [
                            {"match_phrase": {"job_company_name": n}},
                            {"match": {"job_company_name": n}},
                        ]
                    }
                })
        if company_clauses:
            if len(company_clauses) == 1:
                must.append(company_clauses[0])
            else:
                must.append({"bool": {"should": company_clauses}})

    # Schools: match_phrase only with _school_aliases (OR alternatives); no minimum_should_match (PDL doesn't support it).
    schools = parsed_prompt.get("schools") or []
    if schools:
        education_clauses = []
        for school in schools:
            aliases = _school_aliases(school)
            for a in aliases:
                education_clauses.append({"match_phrase": {"education.school.name": a}})
        if education_clauses:
            must.append({"bool": {"should": education_clauses}})

    query_obj = {"bool": {"must": must}}
    if retry_level > 0:
        print(f"[build_query_from_prompt] Retry level {retry_level} query:\n{json.dumps(query_obj, indent=2)}")
    else:
        print(f"[build_query_from_prompt] Full Elasticsearch query:\n{json.dumps(query_obj, indent=2)}")
    return query_obj


def _contact_matches_prompt_criteria(contact, parsed_prompt, target_company):
    """
    Post-validation: return True if contact matches user's company and school criteria.
    Returns (matches: bool, drop_reason: str | None).
    """
    name = f"{contact.get('FirstName', '')} {contact.get('LastName', '')}".strip()
    companies = parsed_prompt.get("companies") or []
    schools = parsed_prompt.get("schools") or []

    # Verbose debug: what we're checking
    contact_company = (contact.get("Company") or "").strip()
    is_current = contact.get("IsCurrentlyAtTarget", False)
    college = (contact.get("College") or "").strip()
    education_top = (contact.get("EducationTop") or "").strip()
    first_job = (contact.get("experience") or [{}])[0] if (contact.get("experience")) else None
    first_job_company = (first_job.get("company", {}) or {}).get("name") or first_job.get("company_name") or ""
    print(f"[PostFilter] Checking contact: {name}")
    print(f"[PostFilter] Target companies: {companies!r}, Contact company: {contact_company!r}, IsCurrentlyAtTarget: {is_current}, first_job_company: {first_job_company!r}")
    print(f"[PostFilter] Target schools: {schools!r}, Contact College: {college!r}, EducationTop: {(education_top[:80] + ('...' if len(education_top) > 80 else ''))!r}")

    # Company check: if user specified a company, contact must be currently at target company
    if companies and target_company:
        if not is_current:
            print(f"[PostFilter] Result for {name}: FAIL ‚Äî not currently at target company (expected={target_company})")
            return False, "not_currently_at_target"
        actual = contact_company
        if not actual:
            print(f"[PostFilter] Result for {name}: FAIL ‚Äî company mismatch (expected={target_company}, got=no company)")
            return False, "company_mismatch"
        cleaned_expected = clean_company_name(target_company).lower().strip()
        cleaned_actual = clean_company_name(actual).lower().strip()
        if cleaned_expected != cleaned_actual and not (cleaned_expected in cleaned_actual and len(cleaned_expected) >= 3):
            print(f"[PostFilter] Result for {name}: FAIL ‚Äî company mismatch (expected={target_company}, got={actual})")
            return False, "company_mismatch"

    # School check: if user specified schools, contact must have at least one matching school
    if schools:
        alias_set = set()
        for school in schools:
            for a in _school_aliases(school):
                if a:
                    alias_set.add(a.lower().strip())
        if not alias_set:
            print(f"[PostFilter] Result for {name}: PASS (no school aliases to check)")
            return True, None
        college_lower = college.lower().strip()
        education_top_lower = education_top.lower().strip()
        combined = f"{college_lower} {education_top_lower}"
        if not combined.strip():
            print(f"[PostFilter] Result for {name}: FAIL ‚Äî school mismatch (expected one of {schools}, got=no education)")
            return False, "school_mismatch"
        if not any(alias in combined or alias in college_lower or college_lower in alias for alias in alias_set):
            print(f"[PostFilter] Result for {name}: FAIL ‚Äî school mismatch (expected one of {schools}, got College={contact.get('College')})")
            return False, "school_mismatch"

    print(f"[PostFilter] Result for {name}: PASS")
    return True, None


def search_contacts_from_prompt(parsed_prompt: dict, max_contacts: int, exclude_keys=None):
    """
    Run PDL person search from structured prompt output. Reuses execute_pdl_search,
    applies exclusion filtering and post-validation (company/school match), returns contacts.
    Post-validation runs AFTER execute_pdl_search and BEFORE contacts are returned (so before emails/drafts).
    """
    print(f"[PostFilter] search_contacts_from_prompt called (max_contacts={max_contacts}, parsed keys={list(parsed_prompt.keys())})")
    exclude_keys = exclude_keys or set()
    target_company = None
    companies = parsed_prompt.get("companies") or []
    if companies and isinstance(companies[0], dict):
        target_company = (companies[0].get("name") or "").strip() or None

    PDL_URL = f"{PDL_BASE_URL}/person/search"
    headers = {
        "Accept": "application/json",
        "Content-Type": "application/json",
        "X-Api-Key": PEOPLE_DATA_LABS_API_KEY,
    }
    fetch_limit = int(min(max_contacts * 2, 50))
    page_size = min(100, max(1, fetch_limit))

    raw_contacts = []
    status_code = 200
    for attempt in range(4):
        if attempt >= 1:
            if attempt == 1:
                print(f"[PDL Retry] Attempt 1: simplify titles only (single broad match on core role); keep company + school + location")
            elif attempt == 2:
                print(f"[PDL Retry] Attempt 2: drop title filter (keep company + school + location)")
            else:
                print(f"[PDL Retry] Attempt 3: drop location filter (keep company + school only)")
        query_obj = build_query_from_prompt(parsed_prompt, retry_level=attempt)
        raw_contacts, status_code = execute_pdl_search(
            headers=headers,
            url=PDL_URL,
            query_obj=query_obj,
            desired_limit=fetch_limit,
            search_type="prompt_search",
            page_size=page_size,
            verbose=False,
            target_company=target_company,
        )
        if status_code != 404:
            break
        # 404: no results for this query; try next relaxation

    if not raw_contacts:
        return []

    companies_from_prompt = parsed_prompt.get("companies") or []
    schools_from_prompt = parsed_prompt.get("schools") or []
    print(f"[PostFilter] Running post-validation on {len(raw_contacts)} contacts (parsed companies={companies_from_prompt!r}, schools={schools_from_prompt!r}, target_company={target_company!r})")

    filtered = []
    post_filter_dropped = 0
    for contact in raw_contacts:
        key = get_contact_identity(contact)
        if key in exclude_keys:
            continue
        matches, _ = _contact_matches_prompt_criteria(contact, parsed_prompt, target_company)
        if not matches:
            post_filter_dropped += 1
            continue
        filtered.append(contact)
    if exclude_keys and (len(raw_contacts) != len(filtered) + post_filter_dropped):
        print(f"üîç Prompt search filtering: raw={len(raw_contacts)}, excluded=already seen, returned={len(filtered)}")
    if post_filter_dropped > 0:
        print(f"[PostFilter] Kept {len(filtered)} contacts after post-validation (dropped {post_filter_dropped} non-matching)")
    # Sort: contacts with LinkedIn URL first (stable sort). Never drop contacts for missing LinkedIn.
    linkedin_val = lambda c: (c.get("LinkedIn") or c.get("linkedin_url") or "").strip()
    filtered.sort(key=lambda c: (0 if linkedin_val(c) else 1))
    return filtered[:max_contacts]


def enrich_linkedin_profile(linkedin_url):
    """Use PDL to enrich LinkedIn profile"""
    try:
        # Check cache first
        cached = get_cached_pdl_data(linkedin_url)
        if cached:
            print(f"Using cached data for: {linkedin_url}")
            return cached
        
        print(f"Enriching LinkedIn profile: {linkedin_url}")
        
        # Clean the LinkedIn URL - FIXED VERSION
        linkedin_url = linkedin_url.strip()
        
        # Remove protocol if present
        linkedin_url = linkedin_url.replace('https://', '').replace('http://', '')
        
        # Remove www. if present
        linkedin_url = linkedin_url.replace('www.', '')
        
        # If it's just the username (no linkedin.com), add the full path
        if not linkedin_url.startswith('linkedin.com'):
            linkedin_url = f'https://www.linkedin.com/in/{linkedin_url}'
        else:
            # If it already has linkedin.com, just add https://
            linkedin_url = f'https://{linkedin_url}'
        
        print(f"Cleaned URL: {linkedin_url}")
        
        # Use PDL Person Enrichment API
        response = requests.get(
            f"{PDL_BASE_URL}/person/enrich",
            params={
                'api_key': PEOPLE_DATA_LABS_API_KEY,
                'profile': linkedin_url,
                'pretty': True
            },
            timeout=30
        )
        
        print(f"PDL API response status: {response.status_code}")
        
        if response.status_code == 200:
            person_data = response.json()
            print(f"PDL response status: {person_data.get('status')}")
            
            if person_data.get('status') == 200 and person_data.get('data'):
                print(f"Successfully enriched profile")
                
                # Extract the data using your existing function
                enriched = extract_contact_from_pdl_person_enhanced(person_data['data'])
                
                if not enriched:
                    print(f"Failed to extract contact data")
                    return None
                
                # Transform to coffee chat format
                coffee_chat_data = {
                    'firstName': enriched.get('FirstName', ''),
                    'lastName': enriched.get('LastName', ''),
                    'jobTitle': enriched.get('Title', ''),
                    'company': enriched.get('Company', ''),
                    'location': f"{enriched.get('City', '')}, {enriched.get('State', '')}",
                    'workExperience': [enriched.get('WorkSummary', '')],
                    'education': [enriched.get('EducationTop', '')],
                    'volunteerWork': [enriched.get('VolunteerHistory', '')] if enriched.get('VolunteerHistory') else [],
                    'linkedinUrl': enriched.get('LinkedIn', ''),
                    'email': enriched.get('Email', ''),
                    'city': enriched.get('City', ''),
                    'state': enriched.get('State', ''),
                    'interests': []
                }
                
                print(f"Caching enriched data for: {linkedin_url}")
                set_pdl_cache(linkedin_url, coffee_chat_data)
                return coffee_chat_data
            else:
                print(f"PDL returned status {person_data.get('status')} - no data found")
                if person_data.get('error'):
                    print(f"PDL error: {person_data.get('error')}")
                return None
        
        elif response.status_code == 404:
            print(f"LinkedIn profile not found in PDL database")
            return None
        elif response.status_code == 402:
            print(f"PDL API: Payment required (out of credits)")
            return None
        elif response.status_code == 401:
            print(f"PDL API: Invalid API key")
            return None
        else:
            print(f"PDL enrichment failed with status {response.status_code}")
            print(f"Response: {response.text[:500]}")
            return None
        
    except requests.exceptions.Timeout:
        print(f"‚è±Ô∏è PDL API timeout for {linkedin_url}")
        return None
    except Exception as e:
        print(f"‚ùå LinkedIn enrichment error: {e}")
        import traceback
        traceback.print_exc()
        return None


def get_pdl_cache_key(linkedin_url):
    """Generate cache key for LinkedIn URL"""
    return hashlib.md5(linkedin_url.encode()).hexdigest()


def get_cached_pdl_data(linkedin_url):
    """Get cached PDL data if available"""
    cache_key = get_pdl_cache_key(linkedin_url)
    if cache_key in pdl_cache:
        cached = pdl_cache[cache_key]
        # Don't expire - keep forever as requested
        print(f"Using cached PDL data for {linkedin_url}")
        return cached['data']
    return None


def set_pdl_cache(linkedin_url, data):
    """Cache PDL data permanently"""
    cache_key = get_pdl_cache_key(linkedin_url)
    pdl_cache[cache_key] = {
        'data': data,
        'timestamp': datetime.now()
    }